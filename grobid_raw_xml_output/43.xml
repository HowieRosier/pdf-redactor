<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning for Wound Tissue Segmentation: A Comprehensive Evaluation using A Novel Dataset</title>
				<funder ref="#_B78uCru #_hUve5Fx #_665Mjak #_RFkxUbz #_jV8fnDA #_ns7Vjv9 #_w6hRPVy #_8ayEmKe #_FSBQPJ2 #_KGXMVFM #_49VGkFn #_x3XXaB3 #_g3qb6CR #_knP9BCr #_TXkCms6">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_VV78MCB">
					<orgName type="full">Segmentation Full image UNet, SegNet</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-02-15">15 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Muhammad</forename><surname>Ashad Kabir</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing, Mathematics and Engineering</orgName>
								<orgName type="institution">Charles Sturt University</orgName>
								<address>
									<postCode>2795</postCode>
									<settlement>Bathurst</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nidita</forename><surname>Roy</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Chittagong University of Engineering and Technology</orgName>
								<address>
									<postCode>4349</postCode>
									<settlement>Chattogram</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Md</roleName><forename type="middle">Ekramul</forename><surname>Hossain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing, Mathematics and Engineering</orgName>
								<orgName type="institution">Charles Sturt University</orgName>
								<address>
									<postCode>2795</postCode>
									<settlement>Bathurst</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Engineering</orgName>
								<orgName type="laboratory">Complex Systems Research Group</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jill</forename><surname>Featherston</surname></persName>
							<email>jillfeatherston@optusnet.com.au</email>
							<affiliation key="aff3">
								<orgName type="department">School of Medicine</orgName>
								<orgName type="institution">Cardiff University</orgName>
								<address>
									<postCode>CF14 4YS</postCode>
									<settlement>Cardiff</settlement>
									<region>Wales</region>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sayed</forename><surname>Ahmed</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Principal Pedorthist</orgName>
								<orgName type="institution">Foot Balance Technology Pty Ltd</orgName>
								<address>
									<postCode>2145</postCode>
									<settlement>Westmead</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">Offloading Clinic</orgName>
								<orgName type="institution" key="instit2">High-Risk Foot Services</orgName>
								<orgName type="institution" key="instit3">Nepean Hospital</orgName>
								<address>
									<postCode>2750</postCode>
									<settlement>Kingswood</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Charles Sturt University</orgName>
								<address>
									<addrLine>Panorama Ave</addrLine>
									<postCode>2795</postCode>
									<settlement>Bathurst</settlement>
									<region>NSW</region>
									<country>Australia. Ph</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning for Wound Tissue Segmentation: A Comprehensive Evaluation using A Novel Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-15">15 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">01E7A5A8B32ADE6666F4D0CE50BAE7BD</idno>
					<idno type="arXiv">arXiv:2502.10652v1[eess.IV]</idno>
					<note type="submission">Preprint submitted to &apos; February 18, 2025</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-08-13T17:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Wound tissue</term>
					<term>diabetic foot ulcer</term>
					<term>deep learning</term>
					<term>machine learning</term>
					<term>segmentation</term>
					<term>classification Granulation</term>
					<term>S -Slough</term>
					<term>N -Necrosis</term>
					<term>Es -Eschar</term>
					<term>Ep -Epethelial</term>
					<term>M -Maceration</term>
					<term>T -Tendon</term>
					<term>B -Bone</term>
					<term>I -Infected *Classified Granulation (G) tissue as Healthy Granulation</term>
					<term>Unhealthy Granulation</term>
					<term>and Hyper Granulation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Chronic wounds represent a major global health challenge, imposing substantial economic and social burdens worldwide. Traditional methods of visually inspecting and identifying wound tissue are often time-intensive, costly, inaccurate, imprecise, and inconsistent, with a high degree of variability between clinicians. Deep learning (DL) techniques have emerged as promising solutions for medical wound tissue segmentation. However, a notable limitation in this field is the lack of publicly available labelled datasets and a standardised performance evaluation of state-of-the-art DL models on such datasets. This study addresses this gap by comprehensively evaluating various DL models for wound tissue segmentation using a novel dataset. We have curated a dataset comprising 147 wound images exhibiting six tissue types: slough, granulation, maceration, necrosis, bone, and tendon. The dataset was meticulously labelled for semantic segmentation employing supervised machine learning techniques. Three distinct labelling formats were developed -full image, patch, and superpixel. Our investigation encompassed a wide array of DL segmentation and classification methodologies, ranging from conventional approaches like UNet, to generative adversarial networks such as cGAN, and modified techniques like FPN+VGG16. Also, we explored DL-based classification methods (e.g., ResNet50) and machine learning-based classification leveraging DL features (e.g., AlexNet+RF). In total, 82 wound tissue segmentation models were derived across the three labelling formats. Our analysis yielded several notable findings, including identifying optimal DL models for each labelling format based on weighted average Dice or F1 scores. Notably, FPN+VGG16 emerged as the top-performing DL model for wound tissue segmentation, achieving a dice score of 82.25%. This study provides a valuable benchmark for evaluating wound image segmentation and classification models, offering insights to inform future research and clinical practice in wound care. The labelled dataset created in this study is available at https://github.com/akabircs/WoundTissue.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Chronic wounds, including pressure ulcers, venous ulcers, diabetic foot ulcers (DFUs), infections, and other longlasting injuries, pose a significant global health issue, carrying immense economic and social burdens <ref type="bibr" target="#b0">[1]</ref>. These wounds often arise from conditions such as diabetes, vascular disease, immobility, and pressure injuries and tend to persist in an inflammatory state, leading to prolonged healing times, increased risk of infection, and high rates of morbidity and mortality <ref type="bibr" target="#b1">[2]</ref>. In the United States alone, over 8.2 million individuals suffer from chronic wounds, accounting for over $28 billion in annual healthcare costs <ref type="bibr" target="#b2">[3]</ref>. The burden is equally severe in Australia, where approximately 420,000 people are affected annually, with treatment expenses exceeding AUD 3.5 billion <ref type="bibr" target="#b3">[4]</ref>. Particularly concerning is the rising incidence of DFUs <ref type="bibr" target="#b4">[5]</ref>, a prevalent complication of diabetes mellitus (DM) <ref type="bibr" target="#b5">[6]</ref> that affects approximately 18.6 million people annually, with 34% of diabetic patients developing a DFU in their lifetime. These ulcers carry an elevated risk of severe complications, such as lower extremity amputations (LEAs), with infection and severe cases leading to amputations in up to 20% of instances <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Consequently, chronic wounds represent a critical healthcare challenge, underscoring the need for effective management to alleviate both individual and global impacts.</p><p>One critical aspect of wound management is accurately identifying and analysing various wound tissue types <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. Wounds typically contain multiple tissue types, each playing a distinct role in healing. Granulation tissue, indicative of new connective tissue formation, marks a positive healing stage, while necrotic tissue, consisting of dead cells, impedes wound closure and requires debridement <ref type="bibr" target="#b12">[13]</ref>. Slough, an intermediate type containing dead cells and bacteria, also hampers healing, while macerated tissue signals excess moisture and may require specialised dressings <ref type="bibr" target="#b13">[14]</ref>. In more severe cases, wounds expose deeper structures like tendons and bones, which increases infection risks and necessitates advanced treatment strategies <ref type="bibr" target="#b14">[15]</ref>. Accurately identifying and quantifying wound tissue types is crucial for monitoring healing progress and ensuring comprehensive, detailed documentation, as emphasised by both the Australian guidelines for wound management <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> and the International Working Group on the Diabetic Foot (IWGDF) guidelines <ref type="bibr" target="#b17">[18]</ref>. This documentation provides a chronological record of wound assessment and serves as a foundation for informed treatment decisions.</p><p>Wound tissue assessment mainly relies on manual techniques, such as visual inspection, ruler-based measurements, or digital planimetry, where clinicians subjectively estimate wound tissue composition <ref type="bibr" target="#b18">[19]</ref>. This subjective evaluation can vary significantly between practitioners, making it challenging to ensure consistency and accuracy. Furthermore, chronic wounds often present irregular shapes, poorly defined boundaries, and colour heterogeneity, complicating reliable tissue assessment. The use of the red-black-yellow colour scale, while typical, lacks the precision needed for consistent and accurate wound monitoring <ref type="bibr" target="#b19">[20]</ref>. These manual and subjective methods are time-consuming, inconsistent, and highly dependent on the clinician's experience, leading to variability in treatment approaches <ref type="bibr" target="#b20">[21]</ref>.</p><p>Such limitations also hinder the scalability of wound care, particularly in high-demand settings. This highlights the need for automated, standardised assessment methods to enhance the efficiency, accuracy, and reliability of wound assessments and treatment planning. Such a method would enable clinicians to analyse the proportion of each tissue type quantitatively over time, facilitating more accurate assessments of healing progress and supporting timely and targeted treatment decisions <ref type="bibr" target="#b21">[22]</ref>.</p><p>Recent advancements in artificial intelligence (AI), handheld imaging devices, and mobile applications hold considerable promise for transforming wound care and addressing current limitations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. AI-driven technologies, especially machine learning (ML) and deep learning (DL) can enhance wound assessment by automating tasks like tissue segmentation, classification, and healing prediction, thereby providing precise and reproducible analyses <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. Several clinical studies have demonstrated that AI can perform wound assessment with accuracy and efficiency comparable to human specialists <ref type="bibr" target="#b32">[33]</ref>, often exceeding them in consistency and efficiency <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. Handheld devices, such as the Silhouette Mobile and InSight systems, offer geometric measurements of wound area and volume but cannot currently classify tissue types within the wound, limiting their utility for comprehensive wound assessment <ref type="bibr" target="#b26">[27]</ref>. Meanwhile, smartphone applications like Tissue Analytics have proven effective in enhancing wound assessment and management by improving documentation, enabling remote monitoring, and reducing the burden of patient travel, but they do not support tissue classification <ref type="bibr" target="#b35">[36]</ref>. The mobile application Swift can reduce wound assessment time by approximately half compared to traditional methods <ref type="bibr" target="#b34">[35]</ref>, but it is limited to classifying only four tissue types <ref type="bibr" target="#b36">[37]</ref> and does not support the identification of bone or tendon tissue. Despite these advancements, gaps remain in developing comprehensive wound assessment solutions that integrate the classification and segmentation of all essential tissue types into a unified framework. Many existing applications focus on a limited range of tissue types or lack access to their datasets for further research, underscoring the need for continued development to maximise AI's potential in wound care.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wound Area Segmentation</head><p>Wound Tissue Segmentation Scope of this study The wound segmentation problem can be broadly categorised into two key areas (illustrated in Figure <ref type="figure" target="#fig_0">1</ref>): wound area segmentation and wound tissue segmentation. Wound area segmentation involves isolating the wound area from the background and surrounding body parts within an image. This initial segmentation step is essential as it provides a foundation for more detailed analysis, including wound tissue segmentation, wound measurement, 3D reconstruction, and wound healing assessment. A substantial body of research has been conducted in wound area segmentation, which can partly be attributed to the availability of publicly labelled datasets, such as those from the DFU segmentation challenges <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> and FUSeg challenge <ref type="bibr" target="#b39">[40]</ref>. These studies have evolved from early edge-based segmentation techniques, such as active contour models <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>, to traditional machine learning approaches using colour and texture feature-based approaches <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>, and more recently to deep learning-based approaches <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref>.</p><p>In contrast, wound tissue segmentation involves a pixel-wise classification of the various tissues (e.g., granulation, slough, etc.) found within the wound bed region. It is a more challenging task that has received less attention than wound area segmentation <ref type="bibr" target="#b36">[37]</ref>. This disparity is primarily due to the absence of a publicly available dataset, a key barrier limiting researchers' ability to benchmark and validate new models effectively. Historically, studies in wound tissue segmentation have evolved from traditional machine learning approaches (e.g., <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref>), which relied heavily on handcrafted features and simpler classifiers, to deep learning (DL) methods (e.g., <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b64">65]</ref>), which can learn complex representations directly from data <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b66">67]</ref>. However, most DL-based studies in wound tissue segmentation have applied only a limited selection of existing DL models, primarily CNNs, without systematically comparing the latest state-of-the-art segmentation frameworks. This lack of comprehensive evaluation means that researchers have yet to determine which architectures might best address the intricate requirements of wound tissue segmentation, such as accurately segmenting various tissue types within irregular wound shapes, imbalanced datasets, and in varying lighting conditions. Furthermore, none of the studies have made their datasets publicly accessible, which creates a significant obstacle to continued research, systematic comparisons, and performance enhancement within this field.</p><p>This study systematically evaluated widely used deep learning (DL)-based segmentation and classification models for wound tissue segmentation, using a newly developed wound image dataset, named WoundTissue. The dataset comprises 147 images annotated with six tissue types commonly found in wounds: granulation, necrosis, slough, maceration, tendon, and bone. To facilitate diverse analyses, we preprocessed the dataset into three forms: full image, patch, and superpixel. For segmentation, we categorised the methods into conventional segmentation approaches (e.g., variations of UNet <ref type="bibr" target="#b67">[68]</ref>), generative adversarial networks (e.g., cGAN <ref type="bibr" target="#b68">[69]</ref>), and modified segmentation architectures (e.g., UNet+VGG16, ResNet+VGG16, and FPN+VGG16). Classification was also organised into two main approaches: DL-based classifiers (e.g., VGG16, ResNet50, and InceptionV3) and machine learning (ML)-based classifiers using DL-extracted features (e.g., VGG16+RF, DenseNet201+SVM, and InceptionV3+KNN). Altogether, the varied dataset configurations led to developing and evaluating 82 unique DL models for wound tissue analyses. To our knowledge, this is the first comprehensive study to conduct a quantified comparison of 82 DL models for wound tissue segmentation and classification, offering insights into optimal model configurations for advancing wound care technology. The contributions of this paper are summarized as follows:</p><p>• We introduced a new wound tissue segmentation dataset comprising six distinct types of wound tissues: granulation, necrosis, slough, maceration, tendon, and bone. Notably, this dataset includes maceration, bone, and tendon tissues previously unavailable in labelled datasets. Experts meticulously labelled these tissues, making this dataset the pioneering publicly accessible resource for labelled wound tissue imagery.</p><p>• We comprehensively assessed state-of-the-art deep learning techniques for image segmentation using our novel wound tissue dataset. This evaluation is presented to establish a benchmark for future research in this domain, addressing a previous limitation where the analysis was restricted to only a limited number of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>This section presents an overview of recent studies that used machine learning and deep learning-based approaches specifically for wound tissue segmentation from digital images. For a comprehensive review of this topic, please refer to the references <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b66">67]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Traditional machine learning-based approaches</head><p>Early studies on wound tissue segmentation primarily used traditional machine learning methods that relied on handcrafted features, such as colour and texture, to segment different wound tissues. For instance, several studies <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref> developed a tissue classification model using a Bayesian classifier to classify three types of tissues -granulation, necrotic, and slough -based on colour and texture features extracted from wound images. Similarly, Veredas et al. <ref type="bibr" target="#b69">[70]</ref> proposed a wound tissue classification method that applied three traditional machine learning techniques, SVM, neural network (NN), and random forest (RF), to the same three tissue types using colour and texture features. Several other studies <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b72">73</ref>] also implemented SVM-based classification methods using colour and texture features. In another study, Chang et al. <ref type="bibr" target="#b73">[74]</ref> introduced a multimodal sensor system for wound assessment, which used colour histograms to extract features from segmented wound images. Using the RF algorithm, these features were classified into four different wound tissues: granulation, slough, eschar, and bone/tendon.</p><p>Despite their effectiveness to a degree, these traditional approaches face significant limitations in generalisability, primarily due to their dependence on manually designed features. Such features often fail to capture the complexity and diversity of wound tissues in real-world clinical settings. Notably, none of these studies considered all six tissue types as we have, nor did they provide publicly available datasets to facilitate reproducibility in further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep learning-based approaches</head><p>Recently, DL approaches have gained attention from researchers for wound tissue classification <ref type="bibr" target="#b21">[22]</ref>. Various approaches have been used to tackle the complexity of tissue differentiation and wound boundary delineation, each with distinct methodologies and limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Full image-based segmentation</head><p>This approach processes the entire image as a single entity. It involves applying segmentation algorithms directly to the complete image, generating a segmentation map where each pixel is classified into a specific category (e.g., wound, non-wound, or other tissue types). Full image segmentation benefits from capturing global context and relationships within the image, which can be crucial for accurate segmentation. However, it can be computationally demanding and memory-intensive, especially with high-resolution images. Popular deep learning models such as UNet <ref type="bibr" target="#b67">[68]</ref> and its variants are often used for full image segmentation.</p><p>The application of full image-based segmentation in wound analysis is demonstrated by Godeiro et al. <ref type="bibr" target="#b74">[75]</ref>, who used models like UNet, SegNet, FCN32, and FCN8 on a dataset of 30 images. This work showed promise for segmenting granulation, slough, and necrosis tissues through the limited dataset size constraints model generalisability and reliability. Rajathi et al. <ref type="bibr" target="#b62">[63]</ref> further advanced this approach with DUTCNet, a model capable of distinguishing more diverse tissue types, including eschar and epithelial tissues, across a larger dataset of 150 images. This model demonstrated potential for capturing more nuanced tissue type differences but did not address high variability across different patient populations. Sarp et al. <ref type="bibr" target="#b75">[76]</ref> introduced a conditional generative adversarial network (CGAN) for simultaneous boundary detection and tissue classification. The CGAN approach uniquely facilitates more complex interactions between tissue types, although it relies on a relatively small dataset of 100 images, which could limit accuracy in more varied real-world scenarios. While full image-based segmentation provides a broad contextual overview, it requires extensive labelled data and computational resources, especially with more sophisticated models like CGANs, which are sensitive to dataset diversity and size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Superpixel-based segmentation</head><p>In contrast to full image-based approaches, superpixel-based segmentation divides the image into clusters of pixels, referred to as superpixels, which share similar characteristics such as colour, texture, or intensity. These superpixels are used as the basic units for segmentation instead of classifying individual pixels. The main advantage of superpixel-based segmentation is the reduction in the number of elements to be classified, leading to faster and more efficient processing. Moreover, superpixels capture essential local structures and preserve boundaries more effectively than traditional pixel-wise methods. This approach effectively balances the need to capture local details and maintain computational efficiency.</p><p>Niri et al. <ref type="bibr" target="#b63">[64]</ref> applied superpixel-based segmentation with models like SegNet, UNet, and FCN variants (FCN8, FCN16, FCN32) on a dataset of 219 images, achieving precise segmentation of wound tissues such as granulation, slough, and necrosis. This approach demonstrated that superpixel-based segmentation can yield accurate results with fewer computational resources than full image processing, making it a feasible solution for clinical applications with limited hardware. Similarly, Blanco et al. <ref type="bibr" target="#b64">[65]</ref> leveraged superpixel-based segmentation for dermatological wound analysis, applying feature extraction models like ResNet and VGG16 on 217 images. These models enhanced classification performance by focusing on specific wound regions, although they are limited by their reliance on welldefined tissue boundaries, which are not always present in ulcer images. Superpixel-based segmentation thus balances processing efficiency and localised accuracy, but it may struggle with more heterogeneous ulcer images where tissue types blend gradually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Patch-based segmentation</head><p>Patch-based segmentation represents an intermediary approach wherein an image is divided into smaller patches, either overlapping or non-overlapping patches, and each patch is processed independently. In patch-based approaches, the segmentation problem is often transformed into a classification problem by dividing the image into smaller patches and classifying each patch according to its tissue type, simplifying the segmentation task into a series of patchlevel classifications. Each patch is then classified or segmented, and the results are subsequently combined to form the final segmentation map. This approach can be advantageous in handling high-resolution images and reducing computational load as the model processes smaller sections of the image at a time. Patch-based segmentation is particularly suitable for training deep learning models on small datasets, effectively increasing the training data size without requiring additional full images. Furthermore, it improves the focus on local features within the image.</p><p>However, this approach may encounter difficulties in capturing the global context and can produce artifacts at the boundaries of the patches. Models like convolutional neural networks (CNNs) <ref type="bibr" target="#b76">[77]</ref> commonly perform segmentation tasks on patches.</p><p>One of the foundational studies using this approach was conducted by Zahia et al. <ref type="bibr" target="#b77">[78]</ref>. Wound images were partitioned into small patches of 5 × 5 pixels, and a CNN was used for tissue classification. This method allowed for high granularity in distinguishing tissue types by concentrating on local features within each patch, though the small patch size necessitates a significant number of patches per image, increasing computational demand. Nejati et al. <ref type="bibr" target="#b78">[79]</ref> expanded upon this approach by combining deep learning with machine learning; they divided wound images into larger 20 × 20 patches and used AlexNet to extract features, followed by an SVM classifier to categorise each patch. This hybrid model demonstrated improved accuracy in classifying tissue types, suggesting that patch size and feature extraction methods can significantly impact classification performance.</p><p>Rajathi et al. <ref type="bibr" target="#b79">[80]</ref> implemented a 4-layer CNN for tissue classification of ulcer images, using the patch-based approach to improve the model's focus on subtle tissue variations within wound images. García-Zapirain et al. <ref type="bibr" target="#b80">[81]</ref> further explored patch-based analysis with a 3D CNN, dividing images into 5 × 5 pixel patches to leverage depth information for pressure ulcer classification. This model emphasised spatial depth features, enhancing the classification of granulation and necrosis tissues but requiring high computational resources. Similarly, Reifs et al. <ref type="bibr" target="#b33">[34]</ref> used four commonly available CNN architectures, employing the same 5 × 5 patch size for their analyses.</p><p>In contrast, Maity et al. <ref type="bibr" target="#b81">[82]</ref> used a pixel-based feature extraction approach, using a 9×9 mask window that scanned over each pixel in the tissue regions to extract relevant features, processed using an autoencoder-based CNN. Lastly, Pholberdee et al. <ref type="bibr" target="#b82">[83]</ref> implemented a simpler CNN architecture that operated on larger 31 × 31 pixel patches. These studies highlight the diversity in tissue segmentation and classification methodologies, emphasising how variations in patch sizes and feature extraction techniques can affect model performance.</p><p>Overall, patch-based segmentation offers flexibility in wound tissue analysis by turning segmentation into a classification task, making it feasible for models to learn from limited data. Patch size, feature extraction method, and CNN architecture selection are all critical to optimising the accuracy of these models. However, this approach may sacrifice some global context, as each patch is classified independently without considering neighbouring patches, which could lead to misclassification in highly heterogeneous wound images. It is also observed that most previous studies considered three tissue types: granulation, necrosis, and slough, such as <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b64">65]</ref>. A few studies included epithelial tissue alongside these three <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b79">80]</ref>. In contrast, our study expanded the segmentation scope to include six tissue types: granulation, necrosis, slough, maceration, tendon, and bone, excluding epithelial tissue, which is often similar to granulation tissue in terms of its role in wound healing.</p><p>Moreover, none of the studies considered both bone and tendon tissues individually in their analysis, like this study.</p><p>Furthermore, none of the existing studies made their datasets publicly available, whereas we provide access to our dataset to facilitate further research in this area.</p><p>Regarding dataset forms, most existing studies used a patch-based approach, while a few studies used full images and superpixels as inputs to their DL models. Our research incorporated all three forms of the dataset -full images, patches, and superpixels. This comprehensive approach allows for a more robust comparison of DL models across varying input types, addressing the limitations observed in previous studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Material and methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset curation</head><p>This study created a new dataset of wound images specifically for ulcer tissue analysis. The images were sourced from multiple databases, including the DFU database <ref type="bibr" target="#b83">[84]</ref> and the Medetec Wound Database <ref type="bibr" target="#b84">[85]</ref>. However, we encountered challenges related to suboptimal image quality, which made it difficult to distinguish between various tissue types and the need to address dataset imbalance. The abundance of granulating tissue and necrotic-like erythema tissue further complicated the analysis. To maintain the correctness and reliability of the dataset, we opted to remove certain images, resulting in a final collection of 147 wound images. This dataset encompasses various types of ulcers, including pressure ulcers, diabetic foot ulcers, and venous ulcers. The wound images feature a range of tissue types, such as granulation, necrosis, slough, maceration, tendon, and bone.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset preparation and labelling</head><p>After collecting the raw dataset, we implemented several steps to prepare it for use in deep learning (DL) models.</p><p>The first step involved segmenting the wound area, distinguishing it from healthy skin, and removing background elements. Eliminating background objects is essential for emphasising tissue features within the wound and simplifying the tasks of tissue segmentation. In this study, we manually segmented the wound areas and replaced the background with a uniform blue. This process resulted in a dataset of segmented wound images optimised for subsequent analysis.</p><p>In the second step, we applied padding to the images to ensure consistent input dimensions required by various models during training. This preprocessing step prevents unnecessary distortion, such as shrinking or stretching, and allows the images to be resized effectively before input into the models.</p><p>In the third step, we conducted detailed tissue annotation of the ulcer images, specifically delineating the various tissue types within the ulcer region. The six selected tissue classes were labelled using the following colour scheme: granulation tissue is represented in red, slough in yellow, maceration in white, necrosis in black, tendon in sky blue, and bone in cream. The background is denoted by blue. This labelling process was critical for the subsequent segmentation task, allowing us to generate precise ground truth data for model training. A team of trained annotators carefully performed the labelling process. To ensure these labels' accuracy and clinical relevance, they were thoroughly reviewed and validated by a qualified medical expert with specialised knowledge in wound care.</p><p>This critical validation step was implemented to minimise labelling errors and maintain the integrity of the dataset used for developing the segmentation model. The resulting dataset is referred to as the 'full image' dataset.</p><p>In the next step, we generated a patched and superpixel form of the full image dataset. The patching form of the dataset includes patches of wound images, where each wound image is divided into patches of size 10 × 10 pixels. We used the Python library patchify <ref type="bibr" target="#b85">[86]</ref> to split images into these smaller patches. The process for creating the patches is illustrated in Figure <ref type="figure" target="#fig_4">3a</ref>. In contrast, the superpixel form of the dataset consists of superpixels, which are defined as groups of pixels with similar characteristics. This study used the simple linear iterative clustering (SLIC) algorithm <ref type="bibr" target="#b86">[87]</ref> to generate superpixels from the segmented wound images. Each image was divided into 100 superpixels, each with an approximate size of 128 × 128 pixels. The process involved in creating the superpixels is shown in Figure <ref type="figure" target="#fig_4">3b</ref>.</p><p>Each patch or superpixel of an image was labelled with one of the tissue classes by comparing it to the ground truth of the corresponding original image. Patches or superpixels containing a significant presence of two or more dominant tissue labels were excluded from the dataset.</p><p>The number of images for each tissue class across these three dataset forms -full image (D1), patch (D2), and superpixel (D3) -is reported in Table <ref type="table" target="#tab_2">2</ref>. D1 is suitable for applying segmentation techniques, D2 is appropriate for classification techniques, while D3 can be used for both segmentation and classification.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Wound tissue segmentation and classification approaches</head><p>This subsection describes the state-of-the-art deep learning (DL) approaches used for wound tissue segmentation and classification. A taxonomy of these approaches is illustrated in Figure <ref type="figure" target="#fig_5">4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Classification approach</head><p>In recent years, deep learning (DL) techniques have been extensively used for wound tissue classification <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b87">88]</ref>. This study uses several prominent DL approaches for wound tissue classification, described below.</p><p>CNNs <ref type="bibr" target="#b76">[77]</ref>   AlexNet <ref type="bibr" target="#b88">[89]</ref> is the first CNN that significantly advanced the understanding of visual data. The architecture comprises eight layers, with the first five consisting of convolutional layers followed by max-pooling layers, and the last three are fully connected layers. It uses RELU activation in each layer, except for the output layer. VGG <ref type="bibr" target="#b89">[90]</ref> is another influential CNN architecture that comprises 13 convolutional layers and three fully connected layers. It classifies images into 1,000 categories and achieved a top-5 test accuracy of 92.7% on the ImageNet Dataset. It has several variants, including VGG11, VGG13, VGG16, and VGG19.</p><p>ResNet <ref type="bibr" target="#b90">[91]</ref> is an advanced neural network that uses a technique known as skip connections, which allows the network to bypass one or more layers and connect directly to the output layer. This design effectively addresses the challenges of vanishing and exploding gradients. By using skip connections, the network can learn residual mappings rather than relying solely on the underlining mappings.</p><p>Inception-V3 <ref type="bibr" target="#b91">[92]</ref> is a CNN model from the Inception family that incorporates several modifications, including the factorisation of convolutions into smaller components (7×7 convolutions), label smoothing, and the use of an auxiliary classifier to propagate class information. DenseNet <ref type="bibr" target="#b92">[93]</ref> is another type of CNN that uses connections between each layer and every other layer in a feed-forward manner. Xception <ref type="bibr" target="#b93">[94]</ref> is a variation of the Inception model that uses depth-wise separable convolutions followed by point-wise convolutions.</p><p>MobileNetV2 <ref type="bibr" target="#b94">[95]</ref> is a CNN architecture that uses depth-wise separable convolutions to construct a lightweight model suitable for training on mobile devices with limited computational resources. EfficientNet <ref type="bibr" target="#b95">[96]</ref> is an enhanced version of MobileNetV2 that introduces a compound scaling module, which effectively scales depth, width, and resolution uniformly. NASNetMobile <ref type="bibr" target="#b96">[97]</ref> is another lightweight CNN architecture designed for mobile devices. It features two main components: the normal cell and the reduction cell. The normal cell generates a feature map of the same dimensions, whereas the reduction cell produces a reduced feature map, decreasing the height and width.</p><p>In addition to the aforementioned DL-based techniques, this study uses several modified classification approaches that combine traditional machine learning (ML) algorithms with DL models. In this hybrid approach, DL models are used to extract features from the wound images, which feed into traditional ML algorithms for the classification of wound tissues. The modified approach integrates various DL models, including VGG, AlexNet, Inception, and DenseNet, along with several traditional ML models, including RF, SVM, and KNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Segmentation approach</head><p>This study uses several prominent DL-based techniques that have been successfully applied for segmentation in wound tissue <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b75">76]</ref> or other domains, which are described below.</p><p>UNet <ref type="bibr" target="#b67">[68]</ref> is the most widely used semantic segmentation model in biomedical image processing. It consists of an encoder that scales the features down to a smaller dimension bottleneck and a decoder that scales them back up to their original dimensions. In this study, we employed different modified versions of UNet, including UNet2D, UNet++ 2D, R2UNet, and Attention 2DUNet.</p><p>LinkNet <ref type="bibr" target="#b97">[98]</ref> is similar to UNet, with the main distinction being that LinkNet uses residual blocks in its encoder and decoder networks instead of conventional convolutional blocks. Similarly, Feature Pyramid Network (FPN) <ref type="bibr" target="#b98">[99]</ref> also resembles UNet, but it incorporates a 1×1 convolution layer and adds features rather than copying and appending them, as is done in the UNet architecture.</p><p>Conditional generative adversarial network (cGAN) <ref type="bibr" target="#b68">[69]</ref> is a type of generative adversarial network (GAN). In a GAN, a generator learns to create new images, while a discriminator learns to differentiate between synthetic and real images. In cGAN, conditions are imposed on both the generator and the discriminator using information from other modalities. This enables the model to learn multimodal mappings from inputs to outputs based on diverse information sources.</p><p>In addition to the aforementioned conventional segmentation techniques, this study uses several modified segmentation techniques. In this approach, we use a combination of segmentation methods and DL models, where DL models function as encoders and conventional segmentation methods serve as base models. The modified segmentation approach incorporates the use of UNet, LinkNet, and FPN as base models, along with VGG16, ResNet152, InceptionV3, DenseNet201, and MobileNetV2 as encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evaluation metrics</head><p>The performance of the wound tissue segmentation and classification models is evaluated using several key metrics, including precision, recall, specificity, dice coefficient (Dice), area under the receiver operating characteristic (ROC) curve (AUC), intersection over union (IoU), and Matthews' correlation coefficient (MCC) <ref type="bibr" target="#b77">[78]</ref>. These metrics provide a comprehensive understanding of the models' performance, particularly in distinguishing between different tissue types. We also computed the weighted average for each of these metrics to account for the class imbalance present in the dataset, particularly for underrepresented tissues like bone and tendon. The weighted average was calculated by multiplying the metric score for each class by the proportion of instances in that class and then summing these values. This approach ensures that the overall performance metrics are more representative of the actual class distributions in the dataset, giving a clearer picture of a model's ability to handle both dominant and rare tissue types.</p><p>The ROC curve is characterised by two parameters: the true positive rate (TPR) and the false positive rate (FPR). The other metrics are calculated based on true positive (TP), true negative (TN), false positive (FP), and false negative (FN). Definitions and equations for these metrics and detailed descriptions of how they are calculated are provided in Table <ref type="table" target="#tab_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Model training</head><p>In this study, we trained 82 distinct deep learning (DL)-based classification and segmentation models for the experimental evaluations of wound tissue segmentation. Transfer learning was used, utilising pre-trained ImageNet weights. The models were implemented in Python using the Keras <ref type="bibr" target="#b100">[100]</ref> and TensorFlow <ref type="bibr" target="#b101">[101]</ref> frameworks, and they were trained on a Lambda server equipped with a single NVIDIA GeForce RTX 2080 Ti GPU to enhance computational efficiency.</p><p>Categorical cross-entropy was used as the loss function for the multi-class segmentation and classification models.</p><p>It measures the difference between the predicted probability distribution of pixel classes and the true class labels (onehot encoded) for each pixel, which can be defined as follows:</p><formula xml:id="formula_0">L CCE = - 1 N N i=1 C c=1 w c • y c i • log(ŷ c i )<label>(1)</label></formula><p>where y i is the ground truth label for pixel i and class c, ŷi is the predicted probability that pixel i belongs to class c, N</p><p>is the total number of pixels, C is the total number of classes, and w c is the weight assigned to class c, which adjusts the contribution of each class to the loss. In semantic segmentation, class weights are often used to address the class imbalance, especially when some classes have significantly more pixels than others in the dataset, like this study. The goal of calculating class weights is to give more importance to underrepresented classes during training. We assigned </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics Equation Definition Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T P T P+FP</head><p>Quantifies the accuracy of correctly identified pixels for a specific class within the segmented image. A higher precision indicates fewer false positives, reflecting a model's ability to precisely segment only the intended class pixels without mis-classifying other regions as part of that class. Recall/True Positive Rate</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T P T P+FN</head><p>Measures the proportion of pixels correctly identified for a particular class relative to the total number of pixels that belong to that class in the ground truth. Higher recall indicates that the model has successfully segmented a greater proportion of the actual area belonging to the target class. Specificity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T N T N+FP</head><p>Represents the accurately predicted negatives ratio. High specificity implies that the model effectively avoids incorrectly segmenting irrelevant areas as part of the target class. Dice/F1 2×T P 2×T P+FP+FN Compares the predicted segmentation and the ground truth overlap. It is defined as twice the area of true positives (TP) divided by the sum of the total number of pixels in both the predicted segmentation and the ground truth. Dice is frequently used in medical image analysis, where a slightly higher sensitivity to overlap is beneficial, especially for small and complex structures. It is also called the F1, as it balances precision and recall in a single measurement. IoU</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T P T P+FP+FN</head><p>Like Dice, IoU measures the overlap between predicted and ground truth segmentations, but it calculates this similarity differently. IoU is calculated by dividing the area of overlap (intersection) between the predicted and actual segmentation (i.e., TP) by the area covered by both (union) of these regions. It is often considered a stricter metric, as it directly measures the proportion of overlap relative to the union, making it less forgiving of false positives and false negatives. MCC</p><formula xml:id="formula_1">T P×T N-FP×FN √ (T P+FP)(T P+FN)(T N+FP)(T N+FN)</formula><p>Widely used to assess classification performance, particularly effective for imbalanced datasets. A high MCC score indicates strong predictive accuracy across all four components of the confusion matrix (i.e., TP, TN, FP, and FN), suggesting that the model identifies both classes without bias. MCC values range from -1 to 1, where 1 represents a perfect prediction, 0 indicates no better than random chance, and -1 corresponds to total disagreement between prediction and ground truth. AUC -Measures the performance of a classification model by evaluating the area under the ROC curve. It represents the probability that a randomly selected positive instance is correctly ranked higher than a randomly selected negative instance. An AUC value closer to 1 indicates excellent model performance, while a value closer to 0.5 suggests that the model is no better than random guessing. AUC is particularly useful for assessing classifier performance in imbalanced datasets, as it considers both the true positive and false positive rates across all classification thresholds.</p><p>class weight using the equation as follows:</p><formula xml:id="formula_2">w c = N C • n c (2)</formula><p>where n c is the number of pixels for class c.</p><p>We used the Adam optimisation algorithm to update the network parameters, which is well-regarded in stochastic optimisation for its faster convergence compared to other optimisation algorithms <ref type="bibr" target="#b102">[102]</ref>. We trained models with an initial learning rate of 1 × 10 -4 , which was reduced by a factor of 0.1 if the validation loss did not improve for five consecutive epochs. The total number of epochs was set to 200 with a batch size of eight; however, early stopping was implemented to terminate the training process when no improvement in validation loss was observed for more than fifteen epochs.</p><p>We used the Intersection over Union (IoU), also called the Jaccard index, as the evaluation metric and implemented a ModelCheckpoint to save the model with the highest IoU, ensuring that the best-performing model was retained.</p><p>The IoU expression presented in Table <ref type="table" target="#tab_5">4</ref> can be adapted to calculate the weighted IoU as follows:</p><formula xml:id="formula_3">Weighted IoU = C c=1 n c • |y c ∩ŷ c | |y c ∪ŷ c | C c=1 n c<label>(3)</label></formula><p>where, y c and ŷc represent the ground truth and predicted binary masks for class c, respectively. The numerator computes the weighted sum of IoUs for each class. The denominator ensures normalisation by the sum of weights. C is the total number of tissue classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental evaluations</head><p>The experimental evaluations in this study aimed to achieve four main objectives: (i) to provide baseline evaluation results for four segmentation and classification techniques applied to our wound dataset; (ii) to assess the impact of class weighting on the performance of classification models dealing with imbalanced data; (iii) tissue-wise performance of the selected segmentation and classification models; and (iv) to present leave-one-out cross-validation results for the selected best-performing models, thereby offering a robust estimate of their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Performance of conventional segmentation models</head><p>This study employed six DL-based conventional segmentation models -UNet, UNet2D, UNet++ 2D, R2Unet, Attention 2DUNet, and cGAN -to evaluate their performance in wound tissue segmentation. Each model was applied to the full image (D1) and superpixel (D3) forms of the dataset, and the average weight of all tissue classes for the evaluation metrics was computed based on the test dataset. The results are reported in Table <ref type="table" target="#tab_6">5</ref>, with the best results for each dataset highlighted in bold.</p><p>The analysis revealed that the weighted average Dice score and IoU for UNet2D were the highest for D1, achieving 68.87% and 54.22%, respectively. Conversely, UNet++ 2D attained the highest Dice score and IoU of 68.07% and 54.44%, respectively, for D2. This study used the Dice score as the criterion for selecting the best segmentation model. Therefore, based on overall performance, it is evident that the UNet++ 2D model achieved the highest Dice score of 68.07% for D3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance of modified-segmentation models</head><p>Fifteen modified segmentation models were used for wound tissue segmentation across both the full image (D1) and superpixel (D3) forms of the dataset. These modified models are constructed from a combination of segmentation models (base models or decoders) and DL classification models (encoders). The segmentation models used included UNet, LinkNet, and Feature Pyramid Network (FPN), while the DL classification models consisted of VGG16, ResNet152, DenseNet201, InceptionV3, and MobileNetV2.</p><p>To evaluate the performance of the modified segmentation models, relevant evaluation metrics were calculated for all tissue classes and subsequently averaged based on class weight. The results are reported in Table <ref type="table" target="#tab_7">6</ref>. The combination of FPN and VGG16 was observed to have a high Dice score and IoU of 76.95% and 62.77%, respectively, for D1. For D3, the highest Dice score and IoU of 52.68% and 37.17%, respectively, were obtained with the LinkNet-VGG16 combination. In terms of overall performance, the FPN-VGG16 model exhibited the highest Dice score of 76.95%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance of DL-based classification models</head><p>To evaluate the DL models for tissue classification, we tested eight DL architectures using the D2 and D3 dataset forms. The models included VGG16, ResNet50, DenseNet201, EfficientNetB7, MobileNetV2, InceptionV3, NAS-NetMobile, and Xception. The results are presented in Table <ref type="table" target="#tab_9">7</ref>.</p><p>It was observed that NASNetMobile achieved a high F1 score of 77.13% for D2, while DenseNet201 attained the highest F1 score of 61.39% for D3. Additionally, the best Matthews correlation coefficient (MCC) score of 65.97% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Performance of modified-classification approaches</head><p>In the modified-classification approaches, DL classification models are used as feature extractors and are combined with traditional ML models as classifiers for wound tissue classification. We used twelve modified-classification approach-based models, where the DL models were AlexNet, VGG16, InceptionV3, and DenseNet201, and the ML models included SVM, RF, and KNN.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Applying class weight</head><p>Our dataset exhibits imbalanced classes. To mitigate this class imbalance, we used class weights, assigning weights to classes based on their sizes. The primary purpose of using class weights is to penalise the misclassification of minority classes (such as bone and tendon) by assigning them higher weights, while simultaneously reducing the weights for majority classes (such as granulation, slough, maceration, and necrosis).</p><p>Table <ref type="table" target="#tab_11">9</ref> reports the results of this approach. They indicate that the class weight approach did not yield significant improvements compared to the results obtained without class weights (Table <ref type="table" target="#tab_9">7</ref>) for both the D2 and D3 datasets.</p><p>Specifically, the highest MCC score was 66.37% for D2 and 46.14% for D3 with class weights, while the highest MCC score without class weights was 65.97% for D2 and 46.4% for D3. A similar pattern was observed for the F1 score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Tissue-wise performance of segmentation and classification models</head><p>Table <ref type="table" target="#tab_12">10</ref> presents the tissue-wise performance for the selected segmentation and classification models across three dataset forms: D1 (full image), D2 (patching), and D3 (superpixel). We selected the two best-performing models for each form of the dataset applied to the segmentation and classification approaches. The results show that segmentation models, particularly in the D1 dataset, excelled at identifying granulation and necrotic tissues. Notably, the FPN+VGG16 model demonstrated superior performance, achieving high Dice scores of 89.6% and 78.32% for granulation and necrosis, respectively. However, across all dataset forms, bone and tendon tissues posed significant challenges, with both UNet++ 2D and LinkNet+VGG16 in the D3 dataset failing to segment these tissues, as reflected by 0% Dice scores for bone and very low scores for tendon.</p><p>In classification tasks, models trained on the D2 dataset showed better performance for granulation and necrotic tissues, with NASNetMobile and AlexNet+RF achieving F1 scores of 70% and 85% for granulation and necrosis, respectively. However, the classification of bone and tendon remained problematic across all datasets, with F1 scores dropping to 0% for bone and minimal scores for tendon. The poor performance of both segmentation and classification models for bone and tendon can be attributed to the severe class imbalance, where these tissues are heavily underrepresented, particularly in the D3 dataset.</p><p>To assess the impact of these underrepresented tissue classes on the overall segmentation performance, we designed an experiment where we ran the best two performing models for the D1 dataset, UNet 2D and FPN+VGG16, exclusively on the slough, granulation, maceration, and necrosis classes, while excluding the bone and tendon classes.</p><p>The rationale behind this experiment was to determine how the poor representation and segmentation of bone and tendon in Table <ref type="table" target="#tab_12">10</ref> might have influenced the models' overall performance. The results of this experiment, presented in Table <ref type="table" target="#tab_13">11</ref>, demonstrate a clear improvement in segmentation performance across the remaining tissue classes when bone and tendon were removed.</p><p>For the UNet 2D model, excluding bone and tendon led to a marked increase in performance. In Table <ref type="table" target="#tab_12">10</ref>, the model struggled to segment bone and tendon, yielding very low Dice scores of 9.01% and 5.49%, respectively.</p><p>However, after removing these challenging tissue classes, the weighted average Dice score increased significantly from 67.87% (Table <ref type="table" target="#tab_6">5</ref>) to 70.98%. This improvement was observed across all remaining tissue classes. For instance, the Dice score for granulation increased slightly from 89.72% (Table <ref type="table" target="#tab_12">10</ref>) to 90.44%, while the performance for The FPN+VGG16 model, which was already performing well (see in Table <ref type="table" target="#tab_12">10</ref>), further benefited from the exclusion of bone and tendon. Its weighted average Dice score rose from 76.95% (Table <ref type="table" target="#tab_7">6</ref>) to 78.82%, reflecting enhanced performance across all remaining tissue types. For example, the granulation Dice score improved from 89.6% to 91.36%, and the maceration Dice score increased significantly from 82.78% to 86.35%. Although there was a slight drop in the necrosis Dice score from 78.32% to 69.75%, the overall improvement in performance highlights the model's capacity to segment well-represented tissues more effectively when not hampered by the underrepresented and poorly segmented bone and tendon classes. Despite all these challenges, the D1 form of the dataset, when paired with the FPN+VGG16 model, demonstrated the best overall performance for wound tissue classification.  In D3, the LinkNet+VGG16 model performed better than the UNet++ 2D, demonstrating strong segmentation capabilities for granulation and necrotic tissues. Despite the lower representation of certain tissue types in the D3 form of the dataset, this model could still identify the major wound tissues with reasonable accuracy. In contrast, UNet++ 2D again exhibited poor segmentation results, particularly in identifying bone and tendon, which were either missed entirely or often classified inaccurately as slough. This suggests that while superpixel-based datasets offer a more compact representation, they might oversimplify the visual features required to accurately segment complex tissue types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Visual performance analysis</head><p>For D2, NASNetMobile demonstrated considerable success in classifying wound tissues, showing strong recognition of granulation, necrosis, and other major tissue types, except for bone and tendon. The visual results show that the model could identify the overall structure of the wound, but finer details, such as blonde and tendon segmentation, were still problematic. This indicates that while NASNetMobile is well-suited for broader tissue classification tasks, it struggles with highly localised and underrepresented classes. On the other hand, the combination of AlexNet with random forest (RF) produced subpar results, as seen in the visual outputs. The predicted images from this model could not correctly identify most wound tissues, resulting in a significant deviation from the ground truth. This highlights the limitations of combining traditional machine learning models with deep learning architectures, particularly for tasks requiring high precision and spatial understanding.</p><p>Overall, the visual analysis in Figure <ref type="figure">5</ref> reinforces the earlier quantitative findings. While FPN+VGG16 emerges as the most effective model for wound tissue segmentation on the D1 form of the dataset, conventional models like UNet2D and UNet++ 2D lag, particularly in classifying bone and tendon tissues. Regarding classification, NASNet-Mobile performs well for D2 but struggles with more complex tissue types, highlighting the need for further refinement in models to handle underrepresented and difficult-to-segment tissues. These findings suggest a clear direction for future research, focusing on improving model architectures for bone and tendon segmentation and exploring more sophisticated approaches for handling imbalanced datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Leave-one-out cross-validation</head><p>This study used leave-one-out cross-validation (LOOCV) on the D1 form of the dataset to evaluate the robustness of deep learning (DL) models for wound tissue segmentation. LOOCV is particularly valuable for estimating accurate model performance in studies with smaller datasets <ref type="bibr" target="#b103">[103]</ref>. We tested the best-performing segmentation model, FPN+VGG16, using LOOCV.</p><p>In LOOCV, the number of folds is equal to the total number of images in the dataset, which in our case was 187.</p><p>Consequently, the experiment was run 187 times, with each iteration using a different single image for testing and the remaining images for training and validation (with 1% of the training set allocated for validation). The final results were calculated as a weighted average over 187 iterations, using the number of pixels as the weight. To assess the effect of class weighting on our highly imbalanced dataset, we conducted LOOCV both with and without applying class weights to categorical cross-entropy as the loss function. Although this shows a modest improvement in segmentation for these classes, the Dice scores for well-represented tissues like granulation, slough, and necrosis decrease a little. These results provide valuable insights into the tradeoffs between class-weighted and non-weighted training approaches. Class weighting improves segmentation accuracy for underrepresented tissues, particularly bone and tendon, which have historically low segmentation performance due to their limited representation in the dataset. However, this improvement comes at the expense of reducing overall accuracy for well-represented classes, as the model must allocate more learning capacity to the difficult-to-segment tissues, which detracts from performance on tissues like granulation and necrosis.  Segmentation vs. classification approach. The main strength of segmentation models is their ability to generate pixelwise predictions, which is crucial for medical imaging tasks like wound tissue analysis, where spatial information and tissue boundaries are paramount. The segmentation models' higher IoU and Dice scores for granulation and necrotic tissues highlight their ability to handle intricate wound textures and details. These models utilize spatial hierarchies and deep feature extraction techniques to capture and localize tissues more effectively. The results show that segmentation models are better suited for medical tasks requiring tissue delineation, which is essential in wound assessment and treatment planning.</p><p>On the other hand, classification models are typically faster and simpler to implement but lack the fine-grained spatial understanding necessary for segmenting complex tissue types. While they performed well in assigning tissue labels to the overall image, their inability to focus on precise regions within the wound area limits their utility for detailed medical diagnosis. They showed strong performance for certain tissues, such as granulation and necrosis, but struggled with more nuanced or less-represented classes like bone and tendon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Limitations</head><p>The primary limitation of this study stems from the small and imbalanced nature of the dataset. The underrepresentation of certain tissue types, particularly bone and tendon, significantly impacted the performance of both segmentation and classification models, leading to poor generalisation for these tissue classes. This imbalance poses a challenge for the models to learn sufficient features for accurate tissue recognition and segmentation, resulting in low-performance metrics for these underrepresented classes. Additionally, the limited size of the dataset restricts the models' ability to generalise to new, unseen data, potentially affecting the reliability of the results when applied in broader clinical settings.</p><p>Another limitation arises from the heterogeneity in image quality. The wound images were collected from various sources, each with different levels of resolution, lighting conditions, and image clarity. This variability in image quality introduces noise and inconsistency, which complicates the training process and may hinder the model's ability to learn robust features. The lack of standardised image acquisition protocols across the dataset may lead to suboptimal performance in real-world applications where high consistency in imaging is required for reliable wound assessment.</p><p>Also, while meticulous effort was made to label wound tissues accurately, agreement among expert physicians on wound labelling can vary substantially, making it challenging to establish a consistent criterion standard <ref type="bibr" target="#b32">[33]</ref>.</p><p>This variability could impact the interpretability and reproducibility of results, particularly when deploying models in diverse clinical environments.</p><p>Despite these challenges, this study marks a significant first step toward leveraging publicly available wound images to develop a labelled wound tissue segmentation dataset. It highlights the feasibility of using these resources to create a labelled dataset for model development, paving the way for future efforts to enhance the dataset in size and quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Future directions</head><p>Augmentation, synthetic data generation and data balancing. The poor performance of the models on underrepresented tissue classes, particularly bone and tendon, highlights the need for enhanced data augmentation, synthetic data generation, and balancing techniques. Advanced augmentation methods such as rotation, flipping, zooming, and colour adjustments can increase the diversity of training samples, exposing models to more varied scenarios and improving their robustness in imbalanced datasets <ref type="bibr" target="#b104">[104]</ref>. Also, synthetic data generation can be a powerful technique to augment rare classes like bone and tendon by creating artificial but realistic images using methods such as the generative adversarial networks (GANs) technique. These synthetic images can help balance the dataset and provide the models with more examples to learn from, especially for underrepresented tissue types <ref type="bibr" target="#b105">[105]</ref>. Alongside these techniques, oversampling or SMOTE (Synthetic Minority Over-sampling Technique) can further address class imbalance, ensuring that the model trains on a more evenly distributed dataset and reducing its bias toward overrepresented classes <ref type="bibr" target="#b106">[106]</ref>. By combining traditional augmentation, synthetic data generation, and data balancing methods, the dataset's representation of bone and tendon can be significantly improved, resulting in better segmentation performance for these challenging tissues.</p><p>Custom loss functions. A promising approach to further improve the recognition of underrepresented tissues is the use of custom loss functions. By assigning higher weights to under-performing classes like bone and tendon, models can be more heavily penalised for misclassifications, encouraging them to focus on these harder-to-classify tissues.</p><p>Loss functions such as focal loss are particularly effective for addressing class imbalance by down-weighting easy examples and emphasising difficult ones <ref type="bibr" target="#b107">[107]</ref>. This method has significantly improved datasets with skewed class distributions <ref type="bibr" target="#b109">[108]</ref>.</p><p>Ensemble and custom models. The varied success of different model architectures across tissue classes suggests that ensemble modelling could be a viable solution to mitigate these discrepancies. By combining the strengths of multiple models-such as UNet++ 2D, FPN+VGG16, and LinkNet+VGG16-an ensemble approach can take advantage of each model's strengths while compensating for their weaknesses. Techniques such as stacking, bagging, or voting allow multiple models to contribute to the final prediction, improving accuracy and robustness, particularly in complex tasks like wound segmentation <ref type="bibr" target="#b110">[109]</ref>. Ensemble models have been widely used in medical image analysis to achieve state-of-the-art results and can be especially effective for improving performance on poorly represented tissue types <ref type="bibr" target="#b111">[110]</ref>.</p><p>One potential approach for a custom model would be to incorporate multi-scale feature extraction modules, allowing the model to capture high-level contextual information and fine details, essential for accurate wound tissue delineation <ref type="bibr" target="#b112">[111]</ref>. Also, attention mechanisms could be integrated to help the model focus on important tissue regions, potentially improving its accuracy on challenging classes <ref type="bibr" target="#b113">[112,</ref><ref type="bibr" target="#b114">113]</ref>. Recent studies (e.g., <ref type="bibr" target="#b115">[114,</ref><ref type="bibr" target="#b116">115]</ref>) have shown that attention-based modules can enhance segmentation performance by guiding the model's focus to regions with higher relevance, which would be particularly useful in medical image analysis where certain tissue classes need more emphasis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This study makes significant contributions by presenting the first publicly available wound tissue segmentation dataset, offering a valuable resource for the research community focused on wound analysis and medical imaging.</p><p>A comprehensive comparison of deep learning (DL) models for wound tissue segmentation and classification was conducted, exploring their performance across different dataset forms, including full image, patch, and superpixel representations. By developing and testing 82 DL models, this study provides an in-depth evaluation of model performance across six critical tissue types: slough, granulation, maceration, necrosis, bone, and tendon.</p><p>One of the major contributions of this work is its rigorous evaluation of model performance using multiple metrics, including precision, recall, Dice score, IoU, AUC and MCC. These comprehensive evaluations allow for a nuanced understanding of each model's strengths and limitations. The results revealed that modified segmentation models, particularly FPN+VGG16, consistently outperformed conventional models, showing their superiority over the full image dataset in handling complex tissue boundaries and structures.</p><p>The significance of this study lies not only in its immediate findings but also in its potential as a benchmark for future research. It also provides extensive discussion highlighting the gaps and potential future work, particularly in addressing the segmentation of bone and tendon tissues. The publicly available dataset and extensive model evaluations can serve as a foundation for further developments in wound tissue segmentation and classification. Researchers can use this dataset to test new algorithms, refine existing models, and explore innovative techniques to improve accuracy, particularly for underrepresented tissue types like bone and tendon. This work sets a strong precedent for addressing real-world challenges in medical imaging, providing both practical insights and a valuable resource for continued advancements in the field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of wound segmentation research and the scope of this study</figDesc><graphic coords="3,124.59,490.36,50.67,50.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2 illustrates wound image samples for each of the six tissue types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Different types of wound tissues in the dataset.</figDesc><graphic coords="9,229.96,339.66,60.16,60.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Patching and superpixels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A taxonomy of different deep learning approaches for wound tissue classification and segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 Figure 5 :</head><label>55</label><figDesc>Figure5provides a visual comparison of the best-performing models' predicted images alongside the corresponding input and ground truth images for the three dataset forms: D1 (full image), D2 (patch), and D3 (superpixel). This visual performance analysis highlights the strengths and limitations of various segmentation and classification models in accurately identifying wound tissues.For D1, the FPN+VGG16 segmentation model demonstrated superior performance by accurately segmenting most wound tissues. The clear delineation of tissue boundaries in the predicted images closely resembles the ground truth, indicating the model's effectiveness in identifying complex tissue structures, such as granulation and necrosis. UNet 2D, however, struggled with more intricate tissue types, particularly bone and tendon. This conventional model failed to correctly classify these tissues, often misclassifying them as slough, which aligns with its poor performance metrics reported earlier. This visual evidence underscores the limitations of standard segmentation models in handling underrepresented tissue classes like bone and tendon, suggesting an opportunity for future model development in this</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :. Discussion 5 . 1 .</head><label>651</label><figDesc>Figure 6: Impact of using class-weight on the segmentation performance of the FPN+VGG16 model evaluated using leave-one-out cross validation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of the DL-based studies</figDesc><table><row><cell>Study</cell><cell>Approach</cell><cell>Input type</cell><cell>Method Model</cell><cell>Tissue</cell><cell>Dataset Image</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>count</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note><p>summarises the above-discussed studies related to deep learning (DL) approaches for wound tissue segmentation and classification, as the main objective of this study is to compare different DL approaches for wound tissue analysis. Notably, existing studies have focused exclusively on either segmentation or classification methods for segmenting wound tissues without considering both approaches. In contrast, our study presents a unique and comprehensive framework incorporating 82 distinct DL models for classification and segmentation approaches.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of WoundTissue dataset.</figDesc><table><row><cell>Dataset form</cell><cell cols="3">Count Granulation Necrosis</cell><cell cols="3">Slough Maceration Tendon</cell><cell>Bone</cell><cell>Total</cell></row><row><cell>Full image (D1)</cell><cell>Image Pixel</cell><cell cols="3">102 4,815,302 812,556 1,735,373 19 74</cell><cell cols="4">25 477,249 73,498 41,074 7,955,052 15 12 147</cell></row><row><cell>Patch (D2)</cell><cell>Image</cell><cell>21,846</cell><cell>7,861</cell><cell>15,548</cell><cell>10,843</cell><cell>448</cell><cell>240</cell><cell>56,786</cell></row><row><cell cols="2">Superpixel (D3) Image</cell><cell>1,802</cell><cell>420</cell><cell>1,041</cell><cell>678</cell><cell>80</cell><cell>18</cell><cell>4,039</cell></row></table><note><p><p><p>Each form of the dataset is split into training, validation, and testing sets, ensuring that each set across all dataset forms includes the same segmented wound images. The number of images, patches, and superpixels allocated for training, validation, and testing is detailed in Table</p>3</p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Summary of the dataset split ratio.</figDesc><table><row><cell cols="4">Dataset form Training Validation Testing</cell><cell>Total</cell></row><row><cell>D1</cell><cell>118</cell><cell>14</cell><cell>15</cell><cell>147</cell></row><row><cell>D2</cell><cell>35196</cell><cell>9966</cell><cell cols="2">31251 76413</cell></row><row><cell>D3</cell><cell>2873</cell><cell>481</cell><cell>685</cell><cell>4039</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>are among the most popular and widely used techniques in DL. The CNN architecture primarily con-</figDesc><table><row><cell></cell><cell cols="2">Wound Tissue Analysis</cell><cell></cell></row><row><cell cols="2">Segmentation</cell><cell>Classification</cell><cell></cell></row><row><cell>approach</cell><cell></cell><cell>approach</cell><cell></cell></row><row><cell>Conventional segmentation</cell><cell>Modified segmentation</cell><cell>ML using DL feature</cell><cell>DL based</cell></row><row><cell>UNet UNet2D UNet++ 2D</cell><cell>UNet + (VGG16, ResNet152, InceptionV3, Densenet201, or MobileNetV2) LinkNet +</cell><cell>VGG16 + (RF, SVM, or KNN) AlexNet + (RF, SVM, or KNN)</cell><cell>VGG16 ResNet50 InceptionV3 DenseNet201</cell></row><row><cell>R2UNet</cell><cell>(VGG16, ResNet152, InceptionV3, Densenet201, or MobileNetV2)</cell><cell>InceptionV3 + (RF, SVM, or KNN)</cell><cell>Xception EfficientNetB7</cell></row><row><cell>Attention 2DUNet</cell><cell>FPN +</cell><cell>DenseNet201 + (RF, SVM, or KNN)</cell><cell>MobileNetV2</cell></row><row><cell>cGAN</cell><cell>(VGG16, ResNet152, InceptionV3,</cell><cell></cell><cell>NASNetMobile</cell></row><row><cell></cell><cell>Densenet201, or MobileNetV2)</cell><cell></cell><cell></cell></row></table><note><p>sists of three layers: convolutional, pooling, and fully connected layers. Convolutional layers contain convolutional</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Definition and equation of the metrics.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance of wound tissue classification models using segmentation models.</figDesc><table><row><cell cols="2">Dataset Model</cell><cell cols="6">Precision (%) Recall (%) Dice (%) IoU (%) AUC MCC (%)</cell></row><row><cell></cell><cell>UNet</cell><cell>47.29</cell><cell>28.59</cell><cell>35.25</cell><cell>24.68</cell><cell>0.92</cell><cell>32.83</cell></row><row><cell></cell><cell>UNet2D</cell><cell>77.84</cell><cell>68.36</cell><cell>67.87</cell><cell>54.22</cell><cell>0.97</cell><cell>67.19</cell></row><row><cell>D1</cell><cell>UNet++ 2D</cell><cell>75.87</cell><cell>69.45</cell><cell>65.56</cell><cell>51.5</cell><cell>0.97</cell><cell>65.88</cell></row><row><cell></cell><cell>R2UNet</cell><cell>64.06</cell><cell>64.49</cell><cell>63.31</cell><cell>48.8</cell><cell>0.95</cell><cell>60.7</cell></row><row><cell></cell><cell>Attention 2DUNet</cell><cell>60.45</cell><cell>68.02</cell><cell>62.3</cell><cell>51.83</cell><cell>0.96</cell><cell>60.83</cell></row><row><cell></cell><cell>cGAN</cell><cell>53.59</cell><cell>28.76</cell><cell>30.21</cell><cell>23.48</cell><cell>-</cell><cell>31.77</cell></row><row><cell></cell><cell>UNet</cell><cell>47.36</cell><cell>61.91</cell><cell>44.44</cell><cell>34.42</cell><cell>0.91</cell><cell>45.01</cell></row><row><cell></cell><cell>UNet2D</cell><cell>54.32</cell><cell>49.16</cell><cell>30.99</cell><cell>19.72</cell><cell>0.9</cell><cell>33.25</cell></row><row><cell>D3</cell><cell>UNet++ 2D</cell><cell>64.63</cell><cell>81.17</cell><cell>68.07</cell><cell>54.44</cell><cell>0.96</cell><cell>67.08</cell></row><row><cell></cell><cell>R2UNet</cell><cell>63.34</cell><cell>63.76</cell><cell>48.49</cell><cell>34.65</cell><cell>0.95</cell><cell>50.86</cell></row><row><cell></cell><cell>Attention 2DUNet</cell><cell>61.25</cell><cell>78.61</cell><cell>61.16</cell><cell>46.56</cell><cell>0.96</cell><cell>61.16</cell></row><row><cell></cell><cell>cGAN</cell><cell>54.07</cell><cell>8.17</cell><cell>12.45</cell><cell>7.12</cell><cell>-</cell><cell>13.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Performance of wound tissue classification models using modified-segmentation models.</figDesc><table><row><cell cols="2">Dataset Model</cell><cell>Backbone</cell><cell cols="6">Precision (%) Recall (%) Dice (%) IoU (%) AUC MCC (%)</cell></row><row><cell>D1</cell><cell>UNet</cell><cell>VGG16</cell><cell>57.78</cell><cell>58.37</cell><cell>57.05</cell><cell>41.2</cell><cell>0.91</cell><cell>54.03</cell></row><row><cell></cell><cell></cell><cell>ResNet152</cell><cell>65.59</cell><cell>60.15</cell><cell>55.54</cell><cell>38.77</cell><cell>0.86</cell><cell>56.79</cell></row><row><cell></cell><cell></cell><cell>DenseNet201</cell><cell>68.2</cell><cell>67.79</cell><cell>66.68</cell><cell>52.3</cell><cell>0.88</cell><cell>65.5</cell></row><row><cell></cell><cell></cell><cell>InceptionV3</cell><cell>62.73</cell><cell>69.97</cell><cell>62.09</cell><cell>47.97</cell><cell>0.89</cell><cell>61.9</cell></row><row><cell></cell><cell></cell><cell>MobileNetV2</cell><cell>23.5</cell><cell>25.69</cell><cell>23.85</cell><cell>14.16</cell><cell>0.57</cell><cell>10.15</cell></row><row><cell></cell><cell cols="2">LinkNet VGG16</cell><cell>57.54</cell><cell>61.77</cell><cell>58.21</cell><cell>42.65</cell><cell>0.87</cell><cell>55.33</cell></row><row><cell></cell><cell></cell><cell>ResNet152</cell><cell>75.24</cell><cell>54.89</cell><cell>57.39</cell><cell>41.83</cell><cell>0.91</cell><cell>58.43</cell></row><row><cell></cell><cell></cell><cell>DenseNet201</cell><cell>20.7</cell><cell>27.56</cell><cell>22.11</cell><cell>13.08</cell><cell>0.55</cell><cell>17.47</cell></row><row><cell></cell><cell></cell><cell>InceptionV3</cell><cell>45.5</cell><cell>53.43</cell><cell>46.28</cell><cell>31.47</cell><cell>0.77</cell><cell>44.46</cell></row><row><cell></cell><cell></cell><cell>MobileNetV2</cell><cell>13.91</cell><cell>33.22</cell><cell>17.77</cell><cell>10.42</cell><cell>0.61</cell><cell>13.89</cell></row><row><cell></cell><cell>FPN</cell><cell>VGG16</cell><cell>83.47</cell><cell>72.34</cell><cell>76.95</cell><cell>62.77</cell><cell>0.98</cell><cell>75.87</cell></row><row><cell></cell><cell></cell><cell>ResNet152</cell><cell>41.09</cell><cell>56.83</cell><cell>46.5</cell><cell>31.34</cell><cell>0.93</cell><cell>43.71</cell></row><row><cell></cell><cell></cell><cell>DenseNet201</cell><cell>61.12</cell><cell>43.45</cell><cell>46.66</cell><cell>32.51</cell><cell>0.91</cell><cell>47.05</cell></row><row><cell></cell><cell></cell><cell>InceptionV3</cell><cell>66.25</cell><cell>60.46</cell><cell>63.17</cell><cell>46.94</cell><cell>0.96</cell><cell>61.01</cell></row><row><cell></cell><cell></cell><cell>MobileNetV2</cell><cell>52.67</cell><cell>61.27</cell><cell>53.09</cell><cell>38.22</cell><cell>0.94</cell><cell>52.18</cell></row><row><cell>D3</cell><cell>UNet</cell><cell>VGG16</cell><cell>54.82</cell><cell>52.05</cell><cell>50.85</cell><cell>35.44</cell><cell>0.86</cell><cell>48.37</cell></row><row><cell></cell><cell></cell><cell>ResNet152</cell><cell>62.53</cell><cell>59.12</cell><cell>49.21</cell><cell>36.17</cell><cell>0.88</cell><cell>51.08</cell></row><row><cell></cell><cell></cell><cell>DenseNet201</cell><cell>33.96</cell><cell>28.35</cell><cell>26.77</cell><cell>15.77</cell><cell>0.81</cell><cell>24.48</cell></row><row><cell></cell><cell></cell><cell>InceptionV3</cell><cell>52.95</cell><cell>56.72</cell><cell>47.66</cell><cell>32.12</cell><cell>0.83</cell><cell>47.58</cell></row><row><cell></cell><cell></cell><cell>MobileNetV2</cell><cell>45.8</cell><cell>39.83</cell><cell>21.75</cell><cell>13.65</cell><cell>0.86</cell><cell>22.08</cell></row><row><cell></cell><cell cols="2">LinkNet VGG16</cell><cell>54.58</cell><cell>54.93</cell><cell>52.68</cell><cell>37.17</cell><cell>0.92</cell><cell>49.99</cell></row><row><cell></cell><cell></cell><cell>ResNet152</cell><cell>50.26</cell><cell>36.22</cell><cell>38.42</cell><cell>27.31</cell><cell>0.84</cell><cell>37.87</cell></row><row><cell></cell><cell></cell><cell>DenseNet201</cell><cell>55.69</cell><cell>59.01</cell><cell>41.94</cell><cell>28.76</cell><cell>0.88</cell><cell>44.65</cell></row><row><cell></cell><cell></cell><cell>InceptionV3</cell><cell>44.11</cell><cell>43.03</cell><cell>30.79</cell><cell>18.69</cell><cell>0.75</cell><cell>32.04</cell></row><row><cell></cell><cell></cell><cell>MobileNetV2</cell><cell>56.59</cell><cell>46.71</cell><cell>43.65</cell><cell>28.08</cell><cell>0.92</cell><cell>44</cell></row><row><cell></cell><cell>FPN</cell><cell>VGG16</cell><cell>51.16</cell><cell>28.45</cell><cell>36.47</cell><cell>22.76</cell><cell>0.88</cell><cell>34.97</cell></row><row><cell></cell><cell></cell><cell>ResNet152</cell><cell>59.74</cell><cell>22.86</cell><cell>28.31</cell><cell>19.27</cell><cell>0.55</cell><cell>29.28</cell></row><row><cell></cell><cell></cell><cell>DenseNet201</cell><cell>21.44</cell><cell>17.02</cell><cell>16.69</cell><cell>10.27</cell><cell>0.68</cell><cell>14.42</cell></row><row><cell></cell><cell></cell><cell>InceptionV3</cell><cell>49.5</cell><cell>37.74</cell><cell>31.61</cell><cell>19.64</cell><cell>0.87</cell><cell>32.82</cell></row><row><cell></cell><cell></cell><cell>MobileNetV2</cell><cell>43.01</cell><cell>30.79</cell><cell>20.91</cell><cell>13.5</cell><cell>0.9</cell><cell>21.86</cell></row><row><cell cols="9">was found for NASNetMobile using D2, and 46.4% for DenseNet201 using D3. In terms of overall performance,</cell></row><row><cell cols="4">NASNetMobile exhibited the highest MCC score of 65.97%.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc>reports the performance of these models. The results indicate that the combination of AlexNet and RF showed high performance across all evaluation metrics for D2. For D3, the combination of DenseNet201 and RF</figDesc><table /><note><p>emerged as the best model, based on the F1 and MCC scores. Overall, the combination of AlexNet and RF achieved the highest F1 score and MCC of 76.19% and 67.88%, respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Performance of wound tissue classification models using DL-based approaches.</figDesc><table><row><cell cols="2">Dataset Model</cell><cell cols="7">Precision (%) Recall (%) Specificity (%) F1 Score (%) IoU (%) AUC MCC (%)</cell></row><row><cell>D2</cell><cell>VGG16</cell><cell>52.44</cell><cell>38.06</cell><cell>85.63</cell><cell>40.63</cell><cell>27.51</cell><cell>0.76</cell><cell>26</cell></row><row><cell></cell><cell>ResNet50</cell><cell>78.43</cell><cell>72.82</cell><cell>88.88</cell><cell>74.13</cell><cell>59.59</cell><cell>0.91</cell><cell>65.15</cell></row><row><cell></cell><cell>DenseNet201</cell><cell>79.42</cell><cell>72.35</cell><cell>85.93</cell><cell>71.37</cell><cell>56.19</cell><cell>0.93</cell><cell>63.82</cell></row><row><cell></cell><cell>EfficientNetB7</cell><cell>53.11</cell><cell>35.66</cell><cell>82.39</cell><cell>28.66</cell><cell>18.17</cell><cell>0.84</cell><cell>19.95</cell></row><row><cell></cell><cell>MobileNetV2</cell><cell>73.03</cell><cell>60.07</cell><cell>88.55</cell><cell>62.53</cell><cell>46.16</cell><cell>0.88</cell><cell>52.39</cell></row><row><cell></cell><cell>InceptionV3</cell><cell>70.18</cell><cell>58.76</cell><cell>87.01</cell><cell>61.75</cell><cell>44.71</cell><cell>0.86</cell><cell>49.2</cell></row><row><cell></cell><cell>NASNetMobile</cell><cell>77.52</cell><cell>74.68</cell><cell>88</cell><cell>74.13</cell><cell>59.3</cell><cell>0.94</cell><cell>65.97</cell></row><row><cell></cell><cell>Xception</cell><cell>64.93</cell><cell>42.08</cell><cell>90.15</cell><cell>44.86</cell><cell>30.56</cell><cell>0.83</cell><cell>36.81</cell></row><row><cell>D3</cell><cell>VGG16</cell><cell>52.34</cell><cell>49.97</cell><cell>77.48</cell><cell>49.42</cell><cell>34.73</cell><cell>0.71</cell><cell>28.19</cell></row><row><cell></cell><cell>ResNet50</cell><cell>57.64</cell><cell>58.29</cell><cell>79.35</cell><cell>56.76</cell><cell>42.18</cell><cell>0.77</cell><cell>38.80</cell></row><row><cell></cell><cell>DenseNet201</cell><cell>63.75</cell><cell>65.26</cell><cell>79.56</cell><cell>61.39</cell><cell>47.25</cell><cell>0.80</cell><cell>46.4</cell></row><row><cell></cell><cell>EfficientNetB7</cell><cell>31.62</cell><cell>50.24</cell><cell>61.41</cell><cell>37.41</cell><cell>27.21</cell><cell>0.63</cell><cell>17.13</cell></row><row><cell></cell><cell>MobileNetV2</cell><cell>59.27</cell><cell>57.55</cell><cell>82.33</cell><cell>56.68</cell><cell>41.72</cell><cell>0.71</cell><cell>40.12</cell></row><row><cell></cell><cell>InceptionV3</cell><cell>54.9</cell><cell>54.83</cell><cell>78.36</cell><cell>52.83</cell><cell>38.38</cell><cell>0.70</cell><cell>32.94</cell></row><row><cell></cell><cell>NASNetMobile</cell><cell>50.18</cell><cell>53.31</cell><cell>72.09</cell><cell>49.82</cell><cell>35.29</cell><cell>0.73</cell><cell>26.82</cell></row><row><cell></cell><cell>Xception</cell><cell>51.18</cell><cell>50.56</cell><cell>75.63</cell><cell>50.2</cell><cell>34.89</cell><cell>0.69</cell><cell>26.86</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Performance of wound tissue classification using modified-classification approaches.</figDesc><table><row><cell cols="2">Dataset DL Feature</cell><cell cols="8">ML Model Precision (%) Recall (%) Specificity (%) F1 Score (%) IoU (%) AUC MCC (%)</cell></row><row><cell>D2</cell><cell>AlexNet</cell><cell>SVM</cell><cell>74.35</cell><cell>62.09</cell><cell>90.9</cell><cell>66.45</cell><cell>50.88</cell><cell>0.9</cell><cell>56.02</cell></row><row><cell></cell><cell></cell><cell>RF</cell><cell>80.7</cell><cell>73.38</cell><cell>92.52</cell><cell>76.19</cell><cell>62.03</cell><cell>0.92</cell><cell>67.88</cell></row><row><cell></cell><cell></cell><cell>KNN</cell><cell>76.82</cell><cell>68.38</cell><cell>90.98</cell><cell>71.91</cell><cell>56.82</cell><cell>0.86</cell><cell>61.83</cell></row><row><cell></cell><cell>VGG16</cell><cell>SVM</cell><cell>69.05</cell><cell>55.73</cell><cell>90.73</cell><cell>59.37</cell><cell>45.13</cell><cell>0.87</cell><cell>48.99</cell></row><row><cell></cell><cell></cell><cell>RF</cell><cell>68.22</cell><cell>58.49</cell><cell>88.37</cell><cell>60.61</cell><cell>45.46</cell><cell>0.86</cell><cell>48.97</cell></row><row><cell></cell><cell></cell><cell>KNN</cell><cell>67.94</cell><cell>57.78</cell><cell>89.3</cell><cell>59.73</cell><cell>45.19</cell><cell>0.77</cell><cell>48.9</cell></row><row><cell></cell><cell>InceptionV3</cell><cell>SVM</cell><cell>69.92</cell><cell>56.62</cell><cell>89.89</cell><cell>60.58</cell><cell>44.96</cell><cell>0.9</cell><cell>49.49</cell></row><row><cell></cell><cell></cell><cell>RF</cell><cell>74.21</cell><cell>62.94</cell><cell>89.84</cell><cell>65.81</cell><cell>49.59</cell><cell>0.9</cell><cell>56.15</cell></row><row><cell></cell><cell></cell><cell>KNN</cell><cell>66.31</cell><cell>48.59</cell><cell>87.68</cell><cell>52.23</cell><cell>36.07</cell><cell>0.78</cell><cell>40.62</cell></row><row><cell></cell><cell>DenseNet201</cell><cell>SVM</cell><cell>69.99</cell><cell>58.32</cell><cell>89.99</cell><cell>62.75</cell><cell>48.83</cell><cell>0.87</cell><cell>51.03</cell></row><row><cell></cell><cell></cell><cell>RF</cell><cell>77.96</cell><cell>68.02</cell><cell>92.15</cell><cell>71.67</cell><cell>57.64</cell><cell>0.9</cell><cell>62.42</cell></row><row><cell></cell><cell></cell><cell>KNN</cell><cell>76.19</cell><cell>66.71</cell><cell>91.55</cell><cell>70.45</cell><cell>56.24</cell><cell>0.85</cell><cell>60.47</cell></row><row><cell>D3</cell><cell>AlexNet</cell><cell>SVM</cell><cell>52.68</cell><cell>53.03</cell><cell>70.87</cell><cell>47.95</cell><cell>34.05</cell><cell>0.71</cell><cell>27.13</cell></row><row><cell></cell><cell></cell><cell>RF</cell><cell>50.57</cell><cell>49.32</cell><cell>65.1</cell><cell>41.84</cell><cell>29.28</cell><cell>0.65</cell><cell>19.9</cell></row><row><cell></cell><cell></cell><cell>KNN</cell><cell>50.05</cell><cell>50</cell><cell>73.54</cell><cell>47.99</cell><cell>33.46</cell><cell>0.66</cell><cell>25.13</cell></row><row><cell></cell><cell>VGG16</cell><cell>SVM</cell><cell>56.08</cell><cell>53.52</cell><cell>78.12</cell><cell>52.8</cell><cell>37.77</cell><cell>0.79</cell><cell>33.14</cell></row><row><cell></cell><cell></cell><cell>RF</cell><cell>57.4</cell><cell>57.51</cell><cell>78.83</cell><cell>56.54</cell><cell>41.24</cell><cell>0.79</cell><cell>37.28</cell></row><row><cell></cell><cell></cell><cell>KNN</cell><cell>53.73</cell><cell>51.59</cell><cell>77.55</cell><cell>50.64</cell><cell>36</cell><cell>0.72</cell><cell>30.24</cell></row><row><cell></cell><cell>InceptionV3</cell><cell>SVM</cell><cell>54.67</cell><cell>52.98</cell><cell>77.25</cell><cell>52.69</cell><cell>37.55</cell><cell>0.70</cell><cell>31.49</cell></row><row><cell></cell><cell></cell><cell>RF</cell><cell>54.41</cell><cell>55.17</cell><cell>74.43</cell><cell>52.29</cell><cell>37.85</cell><cell>0.73</cell><cell>32.3</cell></row><row><cell></cell><cell></cell><cell>KNN</cell><cell>53.64</cell><cell>52.24</cell><cell>74.69</cell><cell>51.05</cell><cell>35.99</cell><cell>0.67</cell><cell>28.97</cell></row><row><cell></cell><cell>DenseNet201</cell><cell>SVM</cell><cell>63.4</cell><cell>60.92</cell><cell>82.56</cell><cell>61.12</cell><cell>45.84</cell><cell>0.81</cell><cell>44.72</cell></row><row><cell></cell><cell></cell><cell>RF</cell><cell>63.58</cell><cell>61.7</cell><cell>82.68</cell><cell>61.42</cell><cell>46.64</cell><cell>0.79</cell><cell>45.67</cell></row><row><cell></cell><cell></cell><cell>KNN</cell><cell>59.02</cell><cell>56.05</cell><cell>81.5</cell><cell>56.25</cell><cell>41.27</cell><cell>0.73</cell><cell>38.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Performance of wound tissue classification models using DL-based approaches by applying class weights.</figDesc><table><row><cell cols="2">Dataset Model</cell><cell cols="7">Precision (%) Recall (%) Specificity (%) F1 Score (%) IoU (%) AUC MCC (%)</cell></row><row><cell>D2</cell><cell>VGG16</cell><cell>59.4</cell><cell>46.74</cell><cell>79.06</cell><cell>46.22</cell><cell>30.5</cell><cell>0.79</cell><cell>30.05</cell></row><row><cell></cell><cell>ResNet50</cell><cell>78.5</cell><cell>74.95</cell><cell>88.55</cell><cell>74.36</cell><cell>60.77</cell><cell>0.91</cell><cell>66.37</cell></row><row><cell></cell><cell>DenseNet201</cell><cell>73.42</cell><cell>53.78</cell><cell>86.18</cell><cell>59.56</cell><cell>43.2</cell><cell>0.83</cell><cell>46.93</cell></row><row><cell></cell><cell>EfficientNetB7</cell><cell>44.26</cell><cell>34.96</cell><cell>96.29</cell><cell>37.55</cell><cell>28.48</cell><cell>0.78</cell><cell>34.28</cell></row><row><cell></cell><cell>MobileNetV2</cell><cell>67.02</cell><cell>32.63</cell><cell>91.46</cell><cell>42.74</cell><cell>28.15</cell><cell>0.81</cell><cell>32.4</cell></row><row><cell></cell><cell>InceptionV3</cell><cell>58.99</cell><cell>36.47</cell><cell>85.66</cell><cell>44.51</cell><cell>29.7</cell><cell>0.74</cell><cell>28.17</cell></row><row><cell></cell><cell>NASNetMobile</cell><cell>81</cell><cell>46.23</cell><cell>96.96</cell><cell>48.79</cell><cell>39.58</cell><cell>0.82</cell><cell>48.2</cell></row><row><cell></cell><cell>Xception</cell><cell>61.97</cell><cell>35.78</cell><cell>87.23</cell><cell>42.1</cell><cell>27.01</cell><cell>0.79</cell><cell>28.96</cell></row><row><cell>D3</cell><cell>VGG16</cell><cell>54.22</cell><cell>42.29</cell><cell>83.41</cell><cell>45.66</cell><cell>30.36</cell><cell>0.73</cell><cell>26.41</cell></row><row><cell></cell><cell>ResNet50</cell><cell>64.19</cell><cell>55.35</cell><cell>85.14</cell><cell>57.05</cell><cell>41.59</cell><cell>0.77</cell><cell>41.52</cell></row><row><cell></cell><cell>DenseNet201</cell><cell>66.64</cell><cell>57.69</cell><cell>88.41</cell><cell>59.20</cell><cell>43.56</cell><cell>0.77</cell><cell>46.14</cell></row><row><cell></cell><cell>EfficientNetB7</cell><cell>43.18</cell><cell>50.33</cell><cell>58.11</cell><cell>37.93</cell><cell>27.06</cell><cell>0.63</cell><cell>17.15</cell></row><row><cell></cell><cell>MobileNetV2</cell><cell>58.86</cell><cell>52.50</cell><cell>85.12</cell><cell>55.15</cell><cell>40.98</cell><cell>0.72</cell><cell>38.33</cell></row><row><cell></cell><cell>InceptionV3</cell><cell>56.29</cell><cell>49.90</cell><cell>82.73</cell><cell>51.83</cell><cell>36.03</cell><cell>0.71</cell><cell>32.61</cell></row><row><cell></cell><cell>NASNetMobile</cell><cell>54.55</cell><cell>40.18</cell><cell>84.86</cell><cell>43.83</cell><cell>29.1</cell><cell>0.74</cell><cell>25.79</cell></row><row><cell></cell><cell>Xception</cell><cell>52.28</cell><cell>39.72</cell><cell>82.46</cell><cell>43.91</cell><cell>29.16</cell><cell>0.69</cell><cell>23.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Performance of segmentation and classification approaches for tissue-wise classification</figDesc><table><row><cell>Approach Dataset</cell><cell>Model</cell><cell>Metrics</cell><cell cols="6">Tissue classes Slough Granulation Bone Tendon Maceration Necrosis</cell></row><row><cell></cell><cell>UNet2D</cell><cell cols="2">Precision (%) 35.65</cell><cell>93.28</cell><cell>24.81</cell><cell>17.33</cell><cell>83.45</cell><cell>92.82</cell></row><row><cell></cell><cell></cell><cell>Recall (%)</cell><cell>84.36</cell><cell>86.43</cell><cell>5.5</cell><cell>3.26</cell><cell>24.85</cell><cell>64.49</cell></row><row><cell></cell><cell></cell><cell>Dice (%)</cell><cell>50.12</cell><cell>89.72</cell><cell>9.01</cell><cell>5.49</cell><cell>38.3</cell><cell>76.1</cell></row><row><cell></cell><cell></cell><cell>IoU (%)</cell><cell>33.44</cell><cell>81.36</cell><cell>4.72</cell><cell>2.82</cell><cell>23.68</cell><cell>61.42</cell></row><row><cell>D1</cell><cell>FPN+VGG16</cell><cell cols="2">MCC (%) Precision (%) 73.32 49.5</cell><cell>86.99 88.12</cell><cell>11.49 83.61</cell><cell>7.09 51.37</cell><cell>42.78 90.18</cell><cell>77.02 94.55</cell></row><row><cell></cell><cell></cell><cell>Recall (%)</cell><cell>71.76</cell><cell>91.14</cell><cell>56.15</cell><cell>95.35</cell><cell>76.5</cell><cell>66.84</cell></row><row><cell>Segmentation</cell><cell>UNet++ 2D</cell><cell cols="2">Dice (%) IoU (%) MCC (%) Precision (%) 35.31 72.53 56.9 70.28 Recall (%) 85.92 Dice (%) 50.05</cell><cell>89.6 81.16 86.55 92.92 84.24 88.37</cell><cell>67.18 50.58 68.4 0 0 0</cell><cell>66.77 50.12 69.59 0 0 0</cell><cell>82.78 70.62 81.29 81.4 23.48 36.45</cell><cell>78.32 64.36 79.18 80.52 76.9 78.67</cell></row><row><cell></cell><cell></cell><cell>IoU (%)</cell><cell>33.38</cell><cell>79.16</cell><cell>0</cell><cell>0</cell><cell>22.29</cell><cell>64.84</cell></row><row><cell>D3</cell><cell cols="3">MCC (%) LinkNet+VGG16 Precision (%) 35.96 49.75</cell><cell>85.37 92.35</cell><cell>0 0</cell><cell>0 12.35</cell><cell>40.9 67.41</cell><cell>78.28 72.91</cell></row><row><cell></cell><cell></cell><cell>Recall (%)</cell><cell>55.14</cell><cell>87.6</cell><cell>0</cell><cell>14.59</cell><cell>37.46</cell><cell>75.93</cell></row><row><cell></cell><cell></cell><cell>Dice (%)</cell><cell>43.53</cell><cell>89.91</cell><cell>0</cell><cell>13.37</cell><cell>48.16</cell><cell>74.39</cell></row><row><cell></cell><cell></cell><cell>IoU (%)</cell><cell>27.82</cell><cell>81.67</cell><cell>0</cell><cell>7.17</cell><cell>31.71</cell><cell>59.22</cell></row><row><cell></cell><cell></cell><cell>MCC (%)</cell><cell>38.8</cell><cell>87.14</cell><cell>-0.09</cell><cell>12.39</cell><cell>46.25</cell><cell>73.89</cell></row><row><cell></cell><cell>NASNetMobile</cell><cell>Precision (%)</cell><cell>68</cell><cell>74</cell><cell>0</cell><cell>100</cell><cell>52</cell><cell>98</cell></row><row><cell></cell><cell></cell><cell>Recall (%)</cell><cell>93</cell><cell>66</cell><cell>0</cell><cell>1</cell><cell>34</cell><cell>60</cell></row><row><cell></cell><cell></cell><cell>F1 Score (%)</cell><cell>79</cell><cell>70</cell><cell>0</cell><cell>2</cell><cell>41</cell><cell>75</cell></row><row><cell></cell><cell></cell><cell>IoU (%)</cell><cell>65.24</cell><cell>53.84</cell><cell>0</cell><cell>0.81</cell><cell>25.74</cell><cell>59.62</cell></row><row><cell>D2</cell><cell>AlexNet+RF</cell><cell>MCC (%) Precision (%)</cell><cell>66.8 78</cell><cell>61.51 71</cell><cell>-0.05 0</cell><cell>8.96 2</cell><cell>41.16 53</cell><cell>72.59 99</cell></row><row><cell></cell><cell></cell><cell>Recall (%)</cell><cell>65</cell><cell>90</cell><cell>0</cell><cell>1</cell><cell>28</cell><cell>74</cell></row><row><cell>Classification</cell><cell>DenseNet201</cell><cell>F1 Score (%) IoU (%) MCC (%) Precision (%) Recall (%) F1 Score (%)</cell><cell>71 54.88 57.66 58 14 23</cell><cell>80 66.01 73.02 70 86 77</cell><cell>0 0 -0.99 0 0 0</cell><cell>1 0.56 0.96 67 27 38</cell><cell>36 22.14 37.51 62 70 66</cell><cell>85 73.35 82.23 36 57 44</cell></row><row><cell></cell><cell></cell><cell>IoU (%)</cell><cell>13.04</cell><cell>62.76</cell><cell>0</cell><cell>23.53</cell><cell>49.12</cell><cell>28.57</cell></row><row><cell>D3</cell><cell cols="2">MCC (%) DenseNet201+RF Precision (%)</cell><cell>22.44 34</cell><cell>54.36 74</cell><cell>0 0</cell><cell>41.41 0</cell><cell>50.8 76</cell><cell>43.46 27</cell></row><row><cell></cell><cell></cell><cell>Recall (%)</cell><cell>53</cell><cell>80</cell><cell>0</cell><cell>0</cell><cell>49</cell><cell>19</cell></row><row><cell></cell><cell></cell><cell>F1 Score (%)</cell><cell>41</cell><cell>77</cell><cell>0</cell><cell>0</cell><cell>60</cell><cell>22</cell></row><row><cell></cell><cell></cell><cell>IoU (%)</cell><cell>25.78</cell><cell>62.56</cell><cell>0</cell><cell>0</cell><cell>42.86</cell><cell>12.5</cell></row><row><cell></cell><cell></cell><cell>MCC (%)</cell><cell>25.09</cell><cell>55.75</cell><cell>0</cell><cell>-0.57</cell><cell>49.98</cell><cell>20.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Performance of segmentation approaches for slough, granulation, maceration, and necrotic tissue classes. showed a notable rise from 38.3% to 59.95%. This indicates that bone and tendon had been negatively impacting the model's overall performance, and once excluded, the model's ability to focus on better-represented classes improved considerably.</figDesc><table><row><cell cols="2">Dataset Model</cell><cell>Metrics</cell><cell cols="4">Tissue type Slough Granulation Maceration Necrosis</cell><cell>Weighted Avg.</cell></row><row><cell>D1</cell><cell>UNet 2D</cell><cell cols="2">Precision (%) 42.29</cell><cell>94</cell><cell>81.41</cell><cell>85.62</cell><cell>70.25</cell></row><row><cell></cell><cell></cell><cell>Recall (%)</cell><cell>76.52</cell><cell>87.14</cell><cell>47.44</cell><cell>68.38</cell><cell>76.61</cell></row><row><cell></cell><cell></cell><cell>Dice (%)</cell><cell>54.48</cell><cell>90.44</cell><cell>59.95</cell><cell>76.03</cell><cell>70.98</cell></row><row><cell></cell><cell></cell><cell>IoU (%)</cell><cell>37.44</cell><cell>82.55</cell><cell>42.81</cell><cell>61.33</cell><cell>57.26</cell></row><row><cell></cell><cell></cell><cell>MCC (%)</cell><cell>52.22</cell><cell>87.91</cell><cell>59.08</cell><cell>76.11</cell><cell>69.3</cell></row><row><cell></cell><cell cols="3">FPN+VGG16 Precision (%) 74.99</cell><cell>89.32</cell><cell>90.95</cell><cell>88.62</cell><cell>83.33</cell></row><row><cell></cell><cell></cell><cell>Recall (%)</cell><cell>75.04</cell><cell>93.49</cell><cell>82.2</cell><cell>57.5</cell><cell>76.09</cell></row><row><cell></cell><cell></cell><cell>Dice (%)</cell><cell>75.02</cell><cell>91.36</cell><cell>86.35</cell><cell>69.75</cell><cell>78.82</cell></row><row><cell></cell><cell></cell><cell>IoU (%)</cell><cell>60.02</cell><cell>84.09</cell><cell>75.98</cell><cell>53.55</cell><cell>65.93</cell></row><row><cell></cell><cell></cell><cell>MCC (%)</cell><cell>72.94</cell><cell>88.82</cell><cell>84.99</cell><cell>70.96</cell><cell>77.49</cell></row></table><note><p>maceration</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12</head><label>12</label><figDesc>summarises the performance of the FPN+VGG16 model using LOOCV for various tissue types. The weighted average values across all tissue types indicate the model's general performance. Without class weighting, the weighted average Dice score was 82.25%, and IoU was 75.45%. With class weighting, these metrics decreased slightly, with the Dice score averaging 78.31%, and IoU 70.14%. Figure 6 directly compares how class weighting influences model performance (Dice score), particularly for underrepresented tissues like bone and tendon. With class weighting applied, the Dice score for bone increases significantly from 24.28% (without class weighting) to 42.94%, and there is a slight improvement in the tendon from 31.76% to 32.55%, and in maceration from 77.06% to 82.48%.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Performance of FPN+VGG16 model evaluated using leave-one-out cross-validation</figDesc><table><row><cell>Dataset Model</cell><cell>Metrics</cell><cell cols="6">Tissue type/condition Slough Granulation Bone Tendon Maceration Necrosis</cell><cell>Weighted Avg.</cell></row><row><cell></cell><cell cols="2">Precision (%) 86.44</cell><cell>95.19</cell><cell>62.35</cell><cell>70.69</cell><cell>90.66</cell><cell>96.21</cell><cell>91.28</cell></row><row><cell>FPN+VGG16 (w/o class weight)</cell><cell>Recall (%) Dice (%) IoU (%)</cell><cell>75.68 78.00 71.00</cell><cell>85.20 88.39 81.83</cell><cell>17.31 24.28 16.41</cell><cell>28.26 31.76 23.52</cell><cell>74.61 77.06 68.40</cell><cell>94.01 94.37 90.47</cell><cell>79.58 82.25 75.45</cell></row><row><cell>D1</cell><cell cols="2">MCC (%) Precision (%) 79.57 75.51</cell><cell>85.54 94.12</cell><cell>28.51 45.71</cell><cell>33.58 64.59</cell><cell>71.87 83.50</cell><cell>93.04 91.99</cell><cell>79.51 83.50</cell></row><row><cell>FPN+VGG16 (with class weight)</cell><cell>Recall (%) Dice (%) IoU (%)</cell><cell>71.48 72.16 63.80</cell><cell>75.90 81.30 72.84</cell><cell>47.71 42.94 31.67</cell><cell>36.02 32.55 22.81</cell><cell>85.83 82.48 74.25</cell><cell>96.66 94.05 89.18</cell><cell>76.41 78.31 70.14</cell></row><row><cell></cell><cell>MCC (%)</cell><cell>67.69</cell><cell>78.16</cell><cell>43.09</cell><cell>34.56</cell><cell>74.41</cell><cell>91.64</cell><cell>74.40</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p><rs type="funder">Segmentation Full image UNet, SegNet</rs>, <rs type="grantNumber">FCN32</rs>, <rs type="grantNumber">FCN8</rs> G, S, N 30 No [<rs type="grantNumber">63</rs>] Segmentation Full image DUTCNet G, Es, N, Ep 150 No [<rs type="grantNumber">76</rs>] Segmentation Full image CGAN G, S, N 100 No [<rs type="grantNumber">64</rs>] <rs type="affiliation">Segmentation Superpixel SegNet, UNet</rs>, <rs type="grantNumber">FCN8</rs>, <rs type="grantNumber">FCN16</rs>, <rs type="grantNumber">FCN32</rs> G, S, N 219 No [<rs type="grantNumber">65</rs>] Classification Superpixel ResNet, <rs type="grantNumber">VGG16</rs>, InceptionV3 G, S, N 217 No [<rs type="grantNumber">78</rs>] Classification Patch CNN G, S, N 22 No [<rs type="grantNumber">80</rs>] Classification Patch CNN G, S, N, Ep 1,250 No [<rs type="grantNumber">81</rs>] Classification Patch 3D CNN G, S, N 193 No [<rs type="grantNumber">83</rs>] Classification Patch CNN G, S, N 180 No [<rs type="grantNumber">82</rs>] Classification Pixel CNN G, S, N 250 No [<rs type="grantNumber">79</rs>] <rs type="affiliation">Classification Patch AlexNet+SVM G*, S, N, Ep, I 350 No [34] Classification Patch VGG16, InceptionResNetV2, InceptionV3, ResNet50 G, S, N 727 No This study Segmentation and Classification Full image, Superpixel, Patch State</rs>-of-the-art 82 DL approaches G, S, N, M, T, B</p></div>
			</div>
			<div type="funding">
<div><head>Funding</head><p>This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_VV78MCB">
					<idno type="grant-number">FCN32</idno>
				</org>
				<org type="funding" xml:id="_B78uCru">
					<idno type="grant-number">FCN8</idno>
				</org>
				<org type="funding" xml:id="_hUve5Fx">
					<idno type="grant-number">63</idno>
				</org>
				<org type="funding" xml:id="_665Mjak">
					<idno type="grant-number">76</idno>
				</org>
				<org type="funding" xml:id="_RFkxUbz">
					<idno type="grant-number">64</idno>
				</org>
				<org type="funding" xml:id="_jV8fnDA">
					<idno type="grant-number">FCN8</idno>
				</org>
				<org type="funding" xml:id="_ns7Vjv9">
					<idno type="grant-number">FCN16</idno>
				</org>
				<org type="funding" xml:id="_w6hRPVy">
					<idno type="grant-number">FCN32</idno>
				</org>
				<org type="funding" xml:id="_8ayEmKe">
					<idno type="grant-number">65</idno>
				</org>
				<org type="funding" xml:id="_FSBQPJ2">
					<idno type="grant-number">VGG16</idno>
				</org>
				<org type="funding" xml:id="_KGXMVFM">
					<idno type="grant-number">78</idno>
				</org>
				<org type="funding" xml:id="_49VGkFn">
					<idno type="grant-number">80</idno>
				</org>
				<org type="funding" xml:id="_x3XXaB3">
					<idno type="grant-number">81</idno>
				</org>
				<org type="funding" xml:id="_g3qb6CR">
					<idno type="grant-number">83</idno>
				</org>
				<org type="funding" xml:id="_knP9BCr">
					<idno type="grant-number">82</idno>
				</org>
				<org type="funding" xml:id="_TXkCms6">
					<idno type="grant-number">79</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data availability statement</head><p>This study utilised publicly available secondary data. The labelled dataset created in this study is partially available at https://github.com/akabircs/WoundTissue. The full labelled dataset will be available after acceptance.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics statement</head><p>This study utilised publicly available secondary data, and we adhered to relevant terms and conditions associated with its use. No new data were collected directly from human participants. Our study complies with all ethical guidelines and regulations concerning the responsible use of secondary data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of competing interest</head><p>The authors declare that they have no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="30,86.76,138.85,444.00,7.12;30,86.76,153.05,232.72,7.12" xml:id="b0">
	<analytic>
		<title level="a" type="main">The humanistic and economic burden of chronic wounds: A systematic review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Järbrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Divakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Upton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schmidtchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Car</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wound repair and regeneration</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="114" to="125" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,167.25,444.00,7.12;30,86.76,181.44,92.75,7.12" xml:id="b1">
	<analytic>
		<title level="a" type="main">Chronic wounds</title>
		<author>
			<persName><forename type="first">V</forename><surname>Falanga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Isseroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Soulika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Romanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Margolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Granick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Harding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Disease Primers</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">50</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,195.64,409.91,7.12" xml:id="b2">
	<analytic>
		<title level="a" type="main">Human wound and its burden: updated 2022 compendium of estimates</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in wound care</title>
		<imprint>
			<biblScope unit="page" from="657" to="670" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,209.84,346.81,7.12" xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Australia</surname></persName>
		</author>
		<title level="m">Pre-budget submission to fight Australia&apos;s chronic wound epidemic, Wounds Australia</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,224.03,444.01,7.12;30,86.76,238.23,412.60,7.12" xml:id="b4">
	<analytic>
		<title level="a" type="main">Global trends in the incidence of hospital admissions for diabetes-related foot disease and amputations: a review of national rates in the 21st century</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Lazzarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Cramb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Golledge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Magliano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Netten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diabetologia</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="267" to="287" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,252.43,444.01,7.12;30,86.76,266.62,444.01,7.12;30,86.76,280.82,90.31,7.12" xml:id="b5">
	<analytic>
		<title level="a" type="main">Idf diabetes atlas: Global, regional and country-level diabetes prevalence estimates for 2021 and projections for 2045</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karuranga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pinkepank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ogurtsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mbanya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diabetes research and clinical practice</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="page">109119</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,295.02,351.54,7.12" xml:id="b6">
	<analytic>
		<title level="a" type="main">Diabetic foot ulcers: a review</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Boulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Bus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="page" from="62" to="75" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,309.21,437.45,7.12" xml:id="b7">
	<analytic>
		<title level="a" type="main">The current burden of diabetic foot disease</title>
		<author>
			<persName><forename type="first">M</forename><surname>Edmonds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of clinical orthopaedics and trauma</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="88" to="93" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,323.41,444.01,7.12;30,86.76,337.61,202.09,7.12" xml:id="b8">
	<analytic>
		<title level="a" type="main">Wound healing and treating wounds: Chronic wound care and management</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Higham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Broussard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Academy of Dermatology</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="607" to="625" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,351.80,444.01,7.12;30,86.76,366.00,33.87,7.12" xml:id="b9">
	<analytic>
		<title level="a" type="main">The diabetic foot: Pathophysiology, evaluation, and treatment</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Bandyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seminars in vascular surgery</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,380.20,318.02,7.12" xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate assessment of different wound tissue types</title>
		<author>
			<persName><forename type="first">T</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wounds Essentials</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="51" to="54" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,394.40,444.01,7.12;30,86.76,408.59,169.04,7.12" xml:id="b11">
	<analytic>
		<title level="a" type="main">Identifying, managing and preventing skin maceration: a rapid review of the clinical evidence</title>
		<author>
			<persName><forename type="first">F</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Giampieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grocott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of wound care</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="159" to="165" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,422.79,444.00,7.12;30,86.76,436.99,444.01,7.12" xml:id="b12">
	<analytic>
		<title level="a" type="main">Identification of types of wound bed tissue as a percentage and total wound area by planimetry in neuropathic and venous ulcers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B C</forename><surname>Alcântara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>De Araújo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M B</forename><surname>Goulart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M B</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Antunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vascular Nursing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,451.18,426.01,7.12" xml:id="b13">
	<analytic>
		<title level="a" type="main">Maceration of the skin and wound bed 1: its nature and causes</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Cutting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of wound care</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="275" to="278" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,465.38,444.01,7.12;30,86.76,479.58,444.01,7.12;30,86.76,493.77,29.89,7.12" xml:id="b14">
	<analytic>
		<title level="a" type="main">The use of a dermal substitute to preserve maximal foot length in diabetic foot wounds with tendon and bone exposure following urgent surgical debridement for acute infection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Clerici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Caminiti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Curci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Quarantiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Faglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Wound Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="176" to="183" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,507.97,285.97,7.12" xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Australia</surname></persName>
		</author>
		<title level="m">Standards for wound prevention and management</title>
		<meeting><address><addrLine>Cambridge Media</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,522.17,444.01,7.12;30,86.76,536.36,444.01,7.12;30,86.76,550.56,444.01,7.12;30,86.76,564.76,255.08,7.12" xml:id="b16">
	<analytic>
		<title level="a" type="main">Foot Disease Guidelines &amp; Pathways Project Twigg Stephen Lazzarini Peter Raspovic Anita Prentice Jenny Commons Robert Fitridge Robert Charles James Cheney Jane, Australian guideline on wound healing interventions to enhance healing of foot ulcers: part of the 2021 Australian evidence-based guidelines for diabetes-related foot disease</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Carville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Lazzarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Prentice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Related</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Foot and Ankle Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,578.95,444.01,7.12;30,86.76,593.15,444.01,7.12;30,86.76,607.35,83.72,7.12" xml:id="b17">
	<analytic>
		<title level="a" type="main">Practical guidelines on the prevention and management of diabetes-related foot disease (iwgdf 2023 update</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Schaper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Netten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Apelqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Bus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fitridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Game</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Monteiro-Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Senneville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Board</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diabetes/Metabolism Research and Reviews</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">3657</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,621.54,444.01,7.12;30,86.76,635.74,444.01,7.12" xml:id="b18">
	<analytic>
		<title level="a" type="main">Digital planimetry results in more accurate wound measurements: A comparison to standard ruler measurements</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andros</surname></persName>
		</author>
		<idno type="DOI">10.1177/193229681000400405</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Diabetes Science and Technology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="799" to="802" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,649.94,444.01,7.12;30,86.76,664.13,443.93,7.12" xml:id="b19">
	<analytic>
		<title level="a" type="main">Inter-and intra-observer (dis) agreement among nurses and doctors to classify colour and exudation of open surgical wounds according to the red-yellow-black scheme</title>
		<author>
			<persName><forename type="first">H</forename><surname>Vermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Ubbink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Schreuder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Lubbers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of clinical nursing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1270" to="1277" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,678.33,444.01,7.12;30,86.76,692.53,223.26,7.12" xml:id="b20">
	<analytic>
		<title level="a" type="main">Wound management materials and technologies from bench to bedside and beyond</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shirzaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-D</forename><surname>Sani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Materials</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,86.76,706.72,399.09,7.12" xml:id="b21">
	<analytic>
		<title level="a" type="main">Future direction of wound care</title>
		<author>
			<persName><forename type="first">S</forename><surname>Martin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cnur.2024.07.011</idno>
	</analytic>
	<monogr>
		<title level="j">Nursing Clinics of North America</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,115.69,444.01,7.12;31,86.76,129.89,319.02,7.12" xml:id="b22">
	<analytic>
		<title level="a" type="main">The role of artificial intelligence technology in the care of diabetic foot ulcers: the past, the present, and the future</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pappachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrabalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Journal of Diabetes</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">1131</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,144.08,444.01,7.12" xml:id="b23">
	<analytic>
		<title level="a" type="main">Artificial intelligence and machine learning in wound care-the wounded machine!</title>
		<author>
			<persName><forename type="first">D</forename><surname>Queen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International wound journal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">311</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,158.28,444.01,7.12;31,86.76,172.48,287.36,7.12" xml:id="b24">
	<analytic>
		<title level="a" type="main">Mobile apps for wound assessment and monitoring: limitations, advancements and opportunities</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Naher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Featherston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Systems</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,186.67,444.01,7.12;31,86.76,200.87,267.56,7.12" xml:id="b25">
	<analytic>
		<title level="a" type="main">Artificial intelligence in wound care: diagnosis, assessment and treatment of hard-to-heal wounds: a narrative review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Rippon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ousey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of wound care</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="229" to="242" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,215.07,444.01,7.12;31,86.76,229.26,101.96,7.12" xml:id="b26">
	<analytic>
		<title level="a" type="main">Wound size imaging: ready for smart assessment and monitoring</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Niri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Treuillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Douzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Castaneda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in wound care</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="641" to="661" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,243.46,444.01,7.12;31,86.76,257.66,227.88,7.12" xml:id="b27">
	<analytic>
		<title level="a" type="main">Artificial intelligence methodologies applied to technologies for screening, diagnosis and care of the diabetic foot: A narrative review</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chemello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Salvatori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morettini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biosensors</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">985</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,271.85,444.01,7.12;31,86.76,286.05,145.87,7.12" xml:id="b28">
	<analytic>
		<title level="a" type="main">A survey of wound image analysis using deep learning: Classification, detection, and segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="79502" to="79515" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,300.25,444.01,7.12;31,86.76,314.44,152.47,7.12" xml:id="b29">
	<analytic>
		<title level="a" type="main">Machine learning in the prevention, diagnosis and management of diabetic foot ulcers: A systematic review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Akrami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2020">2020. 198977-199000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,328.64,444.00,7.12;31,86.76,342.84,319.98,7.12" xml:id="b30">
	<analytic>
		<title level="a" type="main">Leveraging artificial intelligence and decision support systems in hospital-acquired pressure injuries prediction: A comprehensive review</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Toffaha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C E</forename><surname>Simsekler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Omar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page">102560</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,357.04,444.01,7.12;31,86.76,371.23,444.01,7.12;31,86.76,385.43,132.95,7.12" xml:id="b31">
	<analytic>
		<title level="a" type="main">Pressure injury image analysis with machine learning techniques: A systematic review on previous and possible future methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zahia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Garcia Zapirain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sevillano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elmaghraby</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artmed.2019.101742</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">101742</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,399.63,444.01,7.12;31,86.76,413.82,444.01,7.12;31,86.76,428.02,340.85,7.12" xml:id="b32">
	<analytic>
		<title level="a" type="main">Development of a Method for Clinical Evaluation of Artificial Intelligence-Based Digital Wound Assessment Tools</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Petrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Slone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Gillette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Gorenstein</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamanetworkopen.2021.7234</idno>
	</analytic>
	<monogr>
		<title level="j">JAMA Network Open</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="217234" to="e217234" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,442.22,444.01,7.12;31,86.76,456.41,377.61,7.12" xml:id="b33">
	<analytic>
		<title level="a" type="main">Clinical validation of computer vision and artificial intelligence algorithms for wound measurement and tissue classification in wound care</title>
		<author>
			<persName><forename type="first">D</forename><surname>Reifs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Casanova-Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reig-Bolaño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grau-Carrion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informatics in Medicine Unlocked</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">101185</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,470.61,444.01,7.12;31,86.76,484.81,172.36,7.12" xml:id="b34">
	<analytic>
		<title level="a" type="main">A time motion study of manual versus artificial intelligence methods for wound assessment</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Babb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mannion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">271742</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,499.00,444.01,7.12;31,86.76,513.20,444.01,7.12;31,86.76,527.40,71.06,7.12" xml:id="b35">
	<analytic>
		<title level="a" type="main">Reshaping wound care: Evaluation of an artificial intelligence app to improve wound assessment and management amid the covid-19 pandemic</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barakat-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Frotjold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fethney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Coyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Wound Journal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1561" to="1577" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,541.59,444.01,7.12;31,86.76,555.79,444.01,7.12" xml:id="b36">
	<analytic>
		<title level="a" type="main">Fully automated wound tissue segmentation using deep learning on mobile devices: Cohort study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ramachandram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ramirez-Garcialuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martínez-Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Arriaga-Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMIR mHealth and uHealth</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">36977</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,569.99,444.01,7.12;31,86.76,584.18,444.01,7.12;31,86.76,598.38,444.01,7.12;31,86.76,612.58,128.72,7.12" xml:id="b37">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Byra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galdran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A G</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pappachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrabalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dancey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kendrick</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2024.103153</idno>
	</analytic>
	<monogr>
		<title level="m">Diabetic foot ulcers segmentation challenge report: Benchmark and analysis</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">103153</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,626.77,444.01,7.12;31,86.76,640.97,372.46,7.12" xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Kendrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pappachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>O'shea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chacko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.11618</idno>
		<title level="m">Translating clinical delineation of diabetic foot ulcers into machine interpretable segmentation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="31,86.76,655.17,444.01,7.12;31,86.76,669.36,87.22,7.12" xml:id="b39">
	<analytic>
		<title level="a" type="main">Fuseg: The foot ulcer segmentation challenge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahbod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galdran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niezgoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">140</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,683.56,444.01,7.12;31,86.76,697.76,37.86,7.12" xml:id="b40">
	<analytic>
		<title level="a" type="main">An active contour model for measuring the area of leg ulcers</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Plassmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1202" to="1210" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,86.76,711.95,444.01,7.12;32,86.76,115.69,407.70,7.12" xml:id="b41">
	<analytic>
		<title level="a" type="main">Comparison of segmentation methods for melanoma diagnosis in dermoscopy images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Marc ¸al</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mendonc ¸a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yamauchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rozeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,86.76,129.89,444.01,7.12;32,86.76,144.08,347.34,7.12" xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatic segmentation and degree identification in burn color images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wantanajittikul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Auephanwiriyakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Theera-Umpon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koanantakool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 4th 2011 Biomedical Engineering International Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="169" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,86.76,158.28,444.01,7.12;32,86.76,172.48,259.08,7.12" xml:id="b43">
	<analytic>
		<title level="a" type="main">Segmentation of wounds in the combined color-texture feature space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kolesnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Society for Optics and Photonics</title>
		<imprint>
			<biblScope unit="volume">5370</biblScope>
			<biblScope unit="page" from="549" to="556" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
	<note>Image processing</note>
</biblStruct>

<biblStruct coords="32,86.76,186.67,444.01,7.12;32,86.76,200.87,187.19,7.12" xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-dimensional color histograms for segmentation of wounds in images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kolesnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference Image Analysis and Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1014" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,86.76,215.07,444.01,7.12;32,86.76,229.26,126.63,7.12" xml:id="b45">
	<analytic>
		<title level="a" type="main">How robust is the svm wound segmentation?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kolesnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Nordic Signal Processing Symposium-NORSIG 2006</title>
		<meeting>the 7th Nordic Signal Processing Symposium-NORSIG 2006</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="50" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,86.76,243.46,444.01,7.12;32,86.76,257.66,263.86,7.12" xml:id="b46">
	<analytic>
		<title level="a" type="main">Automated wound identification system based on image segmentation and artificial neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sacan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on bioinformatics and biomedicine</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,86.76,271.85,444.01,7.12;32,86.76,286.05,444.01,7.12;32,86.76,300.25,187.29,7.12" xml:id="b47">
	<analytic>
		<title level="a" type="main">A unified framework for automatic wound segmentation and analysis with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kochhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wrobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 37th annual international conference of the ieee engineering in medicine and biology society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2415" to="2418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,86.76,314.44,444.01,7.12;32,86.76,328.64,436.98,7.12" xml:id="b48">
	<analytic>
		<title level="a" type="main">A framework of wound segmentation based on deep convolutional networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 10th international congress on image and signal processing, biomedical engineering and informatics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,86.76,342.84,444.01,7.12;32,86.76,357.04,339.47,7.12" xml:id="b49">
	<analytic>
		<title level="a" type="main">Segmenting skin ulcers and measuring the wound area using deep convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Chino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Scabora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Cazzolato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Traina-Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Traina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">191</biblScope>
			<biblScope unit="page">105376</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,86.76,371.23,444.01,7.12;32,86.76,385.43,409.20,7.12" xml:id="b50">
	<analytic>
		<title level="a" type="main">Effectiveness of semisupervised active learning in automated wound image segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Curti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Merli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zengarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Giampieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Merlotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dall'olio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marcelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Castellani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Molecular Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">706</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,86.76,399.63,444.01,7.12;32,86.76,413.82,444.01,7.12;32,86.76,428.02,53.13,7.12" xml:id="b51">
	<analytic>
		<title level="a" type="main">Software-based method for automated segmentation and measurement of wounds on photographs using mask r-cnn: A validation study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Privalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Beisemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Barbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mandelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Syrek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Grützner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="788" to="797" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,86.76,442.22,444.01,7.12;32,86.76,456.41,291.87,7.12" xml:id="b52">
	<analytic>
		<title level="a" type="main">Early explorations of lightweight models for wound segmentation on mobile devices</title>
		<author>
			<persName><forename type="first">V</forename><surname>Borst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dittus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kounev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Artificial Intelligence (Künstliche Intelligenz)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="282" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,86.76,470.61,444.00,7.12;32,86.76,484.81,283.74,7.12" xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="32,86.76,499.00,444.01,7.12;32,86.76,513.20,282.22,7.12" xml:id="b54">
	<analytic>
		<title level="a" type="main">Fully automatic wound segmentation with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anisuzzaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rostami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niezgoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,86.76,527.40,444.01,7.12;32,86.76,541.59,366.16,7.12" xml:id="b55">
	<analytic>
		<title level="a" type="main">Superpixel classification with color and texture features for automated wound area segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F A</forename><surname>Fauzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Abas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Student Conference on Research and Development (SCOReD)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,86.76,555.79,444.01,7.12;32,86.76,569.99,200.98,7.12" xml:id="b56">
	<analytic>
		<title level="a" type="main">Automated wound segmentation and classification of seven common injuries in forensic medicine</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sieberth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dobay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Forensic Science, Medicine and Pathology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="443" to="451" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,86.76,584.18,444.01,7.12;32,86.76,598.38,360.20,7.12" xml:id="b57">
	<analytic>
		<title level="a" type="main">Semantic segmentation of smartphone wound images: Comparative analysis of ahrf and cnn-based approaches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Strong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="181590" to="181604" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,86.76,612.58,444.01,7.12;32,86.76,626.77,229.90,7.12" xml:id="b58">
	<monogr>
		<title level="m" type="main">Automated tissue classification framework for reproducible chronic wound assessment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Achar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chakraborty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>BioMed research international 2014</note>
</biblStruct>

<biblStruct coords="32,86.76,640.97,444.01,7.12;32,86.76,655.17,201.87,7.12" xml:id="b59">
	<analytic>
		<title level="a" type="main">Telemedicine supported chronic wound tissue prediction using classification approaches</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medical systems</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,86.76,669.36,444.01,7.12;32,86.76,683.56,172.16,7.12" xml:id="b60">
	<analytic>
		<title level="a" type="main">Binary tissue classification on wound images with neural networks and Bayesian classifiers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Veredas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Morente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="410" to="427" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,86.76,697.76,444.01,7.12;32,86.76,711.95,131.04,7.12" xml:id="b61">
	<analytic>
		<title level="a" type="main">Robust tissue classification for reproducible wound assessment in telemedicine environments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Treuillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">23002</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,115.69,444.01,7.12;33,86.76,129.89,397.21,7.12" xml:id="b62">
	<analytic>
		<title level="a" type="main">DUTC Net: A novel deep ulcer tissue classification network with stage prediction and treatment plan recommendation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Rajathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chinnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Selvakumari</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bspc.2023.105855</idno>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">105855</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,144.08,444.01,7.12;33,86.76,158.28,309.06,7.12" xml:id="b63">
	<analytic>
		<title level="a" type="main">A superpixel-wise fully convolutional neural network approach for diabetic foot ulcer tissue classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Niri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Douzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Treuillet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="308" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,172.48,444.01,7.12;33,86.76,186.67,391.06,7.12" xml:id="b64">
	<analytic>
		<title level="a" type="main">A superpixel-driven deep learning approach for the analysis of dermatological wounds</title>
		<author>
			<persName><forename type="first">G</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Traina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Traina</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Azevedo-Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Bedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer methods and programs in biomedicine</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="page">105079</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,200.87,444.01,7.12;33,86.76,215.07,207.64,7.12" xml:id="b65">
	<analytic>
		<title level="a" type="main">Image-based artificial intelligence in wound assessment: a systematic review</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anisuzzaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rostami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niezgoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Wound Care</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="687" to="709" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,229.26,444.01,7.12;33,86.76,243.46,17.93,7.12" xml:id="b66">
	<analytic>
		<title level="a" type="main">A systematic overview of recent methods for non-contact chronic wound analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marijanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Filko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">7613</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,257.66,444.01,7.12;33,86.76,271.85,293.82,7.12" xml:id="b67">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,286.05,332.07,7.12" xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="33,86.76,300.25,444.01,7.12;33,86.76,314.44,123.08,7.12" xml:id="b69">
	<analytic>
		<title level="a" type="main">Wound image evaluation with machine learning</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Veredas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Luque-Baena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Martín-Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Morilla-Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Morente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="112" to="122" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,328.64,444.01,7.12;33,86.76,342.84,169.79,7.12" xml:id="b70">
	<analytic>
		<title level="a" type="main">Enhanced assessment of the wound-healing process by accurate multiview tissue classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Treuillet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="315" to="326" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,357.04,444.01,7.12;33,86.76,371.23,387.31,7.12" xml:id="b71">
	<analytic>
		<title level="a" type="main">Supervised tissue classification from color images for a complete wound assessment tool</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Treuillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007 29. 2007</date>
			<biblScope unit="page" from="6031" to="6034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,385.43,444.01,7.12;33,86.76,399.63,229.10,7.12" xml:id="b72">
	<analytic>
		<title level="a" type="main">Wound tissue segmentation by computerised image analysis of clinical pressure injury photographs: A pilot study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zamarripa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Wound Care</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="710" to="719" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,413.82,444.01,7.12;33,86.76,428.02,353.12,7.12" xml:id="b73">
	<analytic>
		<title level="a" type="main">Multimodal sensor system for pressure ulcer wound assessment and care</title>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nagraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rajiv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Priebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1186" to="1196" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,442.22,444.01,7.12;33,86.76,456.41,434.59,7.12" xml:id="b74">
	<analytic>
		<title level="a" type="main">Chronic wound tissue classification using convolutional networks and color space reduction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Godeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 28th International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,470.61,444.01,7.12;33,86.76,484.81,218.82,7.12" xml:id="b75">
	<analytic>
		<title level="a" type="main">Simultaneous wound border segmentation and tissue classification using a conditional generative adversarial network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kuzlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pipattanasomporn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Guler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Engineering</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,499.00,444.01,7.12;33,86.76,513.20,422.18,7.12" xml:id="b76">
	<analytic>
		<title level="a" type="main">Review of deep learning: Concepts, cnn architectures, challenges, applications, future directions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alzubaidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Humaidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Dujaili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Al-Shamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Santamaría</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Fadhel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Amidie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Farhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of big Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="74" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,527.40,444.01,7.12;33,86.76,541.59,273.08,7.12" xml:id="b77">
	<analytic>
		<title level="a" type="main">Tissue classification and segmentation of pressure injuries using convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zahia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sierra-Sosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Garcia-Zapirain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elmaghraby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer methods and programs in biomedicine</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="51" to="58" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,555.79,444.01,7.12;33,86.76,569.99,444.01,7.12;33,86.76,584.18,49.81,7.12" xml:id="b78">
	<analytic>
		<title level="a" type="main">Fine-grained wound tissue analysis using deep neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nejati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Ghazijahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdollahzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Malekzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-L</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1010" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,598.38,444.01,7.12;33,86.76,612.58,192.27,7.12" xml:id="b79">
	<analytic>
		<title level="a" type="main">Varicose ulcer (c6) wound image tissue classification using multidimensional convolutional neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Rajathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhavani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Wiselin</forename><surname>Jiji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Imaging Science Journal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="374" to="384" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,626.77,444.01,7.12;33,86.76,640.97,225.14,7.12" xml:id="b80">
	<analytic>
		<title level="a" type="main">Classification of pressure ulcer tissues with 3d convolutional neural network</title>
		<author>
			<persName><forename type="first">B</forename><surname>García-Zapirain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elmogy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El-Baz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Elmaghraby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical &amp; biological engineering &amp; computing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="2245" to="2258" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,655.17,444.01,7.12;33,86.76,669.36,335.12,7.12" xml:id="b81">
	<analytic>
		<title level="a" type="main">Pixel-based supervised tissue classification of chronic wound images with deep autoencoder</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chatterjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Computational and Communication Paradigms</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="727" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,683.56,444.01,7.12;33,86.76,697.76,421.21,7.12" xml:id="b82">
	<analytic>
		<title level="a" type="main">Study of chronic wound image segmentation: Impact of tissue type and color data augmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pholberdee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pathompatai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Taeprasartsit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 15th International Joint Conference on Computer Science and Software Engineering (JCSSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,86.76,711.95,444.01,7.12;34,86.76,115.69,273.12,7.12" xml:id="b83">
	<analytic>
		<title level="a" type="main">Dfu qutnet: diabetic foot ulcer classification using novel deep convo-lutional neural network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alzubaidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Fadhel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Oleiwi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Al-Shamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="15655" to="15677" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,129.89,140.48,7.12" xml:id="b84">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Medetec wound database</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,144.08,241.81,7.15" xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patchify</forename></persName>
		</author>
		<ptr target="https://pypi.org/project/patchify/" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,158.28,336.94,7.12" xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
		<title level="m">Slic superpixels</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="34,86.76,172.48,444.00,7.12;34,86.76,186.67,402.38,7.12" xml:id="b87">
	<analytic>
		<title level="a" type="main">Tissues classification for pressure ulcer images based on 3d convolutional neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elmogy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>García-Zapirain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elmaghraby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ei-Baz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3139" to="3143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,200.87,444.01,7.12;34,86.76,215.07,96.52,7.12" xml:id="b88">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,229.26,441.03,7.12" xml:id="b89">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="34,86.76,243.46,444.01,7.12;34,86.76,257.66,139.68,7.12" xml:id="b90">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,271.85,444.01,7.12;34,86.76,286.05,267.40,7.12" xml:id="b91">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,300.25,444.01,7.12;34,86.76,314.44,211.19,7.12" xml:id="b92">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,328.64,444.01,7.12;34,86.76,342.84,147.65,7.12" xml:id="b93">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,357.04,444.01,7.12;34,86.76,371.23,267.40,7.12" xml:id="b94">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct coords="34,86.76,385.43,444.01,7.12;34,86.76,399.63,95.43,7.12" xml:id="b95">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efficientnet</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,413.82,297.92,7.12" xml:id="b96">
	<monogr>
		<title level="m" type="main">Review: Nasnet-neural architecture search network (image classification</title>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Tsang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,428.02,444.01,7.12;34,86.76,442.22,222.92,7.12" xml:id="b97">
	<analytic>
		<title level="a" type="main">Exploiting encoder representations for efficient semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linknet</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visual Communications and Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,456.41,15.65,7.12" xml:id="b98">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T.-Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="34,104.62,456.41,426.15,7.12;34,86.76,470.61,267.40,7.12" xml:id="b99">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,484.81,258.88,7.15" xml:id="b100">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
	</analytic>
	<monogr>
		<title level="j">Keras</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,499.00,444.01,7.12;34,86.76,513.20,311.55,7.12" xml:id="b101">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="34,86.76,527.40,341.86,7.12" xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="34,86.76,541.59,347.38,7.12" xml:id="b103">
	<analytic>
		<title level="a" type="main">Leave-one-out cross-validation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sammut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Encyclopedia of machine learning</title>
		<imprint>
			<biblScope unit="page" from="600" to="601" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,555.79,399.27,7.12" xml:id="b104">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,569.99,444.01,7.12;34,86.76,584.18,254.12,7.12" xml:id="b105">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,598.38,444.01,7.12;34,86.76,612.58,130.81,7.12" xml:id="b106">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,626.77,15.65,7.12" xml:id="b107">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T.-Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="34,104.06,626.77,426.70,7.12;34,86.76,640.97,112.87,7.12" xml:id="b108">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,655.17,444.01,7.12;34,86.76,669.36,96.52,7.12" xml:id="b109">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,683.56,444.01,7.12;34,86.76,697.76,17.93,7.12" xml:id="b110">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on multiple classifier systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,86.76,711.95,444.01,7.12;35,86.76,115.69,102.71,7.12" xml:id="b111">
	<analytic>
		<title level="a" type="main">Ensemble deep learning: A review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ganaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tanveer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">105151</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,86.76,129.89,441.42,7.12" xml:id="b112">
	<analytic>
		<title level="a" type="main">Deep multi-scale attentional features for medical image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">107445</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,86.76,144.08,444.01,7.12;35,86.76,158.28,63.09,7.12" xml:id="b113">
	<analytic>
		<title level="a" type="main">Multi-scale self-guided attention for medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="121" to="130" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,86.76,172.48,444.01,7.12" xml:id="b114">
	<analytic>
		<title level="a" type="main">Msu-net: Multi-scale u-net for 2d medical image segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Genetics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">639930</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,86.76,186.67,444.01,7.12;35,86.76,200.87,209.79,7.12" xml:id="b115">
	<analytic>
		<title level="a" type="main">Hybrid multiple attention network for semantic segmentation in aerial images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,86.76,215.07,444.01,7.12;35,86.76,229.26,258.57,7.12" xml:id="b116">
	<analytic>
		<title level="a" type="main">Rethinking convolutional attention design for semantic segmentation</title>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Segnext</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1140" to="1156" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
