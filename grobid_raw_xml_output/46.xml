<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classification of pressure ulcer tissues with 3D convolutional neural network</title>
				<funder ref="#_7WRqcVM">
					<orgName type="full">Spanish Ministry</orgName>
				</funder>
				<funder ref="#_5zN6DAK">
					<orgName type="full">Basque Government</orgName>
				</funder>
				<funder>
					<orgName type="full">University of Deusto</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bego</forename><surname>Ña García-Zapirain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facultad Ingeniería</orgName>
								<orgName type="institution">Universidad de Deusto</orgName>
								<address>
									<addrLine>Avda/Universidades 24</addrLine>
									<postCode>48007</postCode>
									<settlement>Bilbao</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">•</forename><forename type="middle">Mohammed</forename><surname>Elmogy</surname></persName>
							<email>melmogy@mans.edu.eg</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Information Technology Department</orgName>
								<orgName type="department" key="dep2">Faculty of Computers and Information</orgName>
								<orgName type="institution">Mansoura University</orgName>
								<address>
									<postCode>35516</postCode>
									<settlement>Mansoura</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Bioengineering Department</orgName>
								<orgName type="institution">University of Louisville</orgName>
								<address>
									<postCode>40292</postCode>
									<settlement>Louisville</settlement>
									<region>KY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ayman</forename><surname>El-Baz</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Bioengineering Department</orgName>
								<orgName type="institution">University of Louisville</orgName>
								<address>
									<postCode>40292</postCode>
									<settlement>Louisville</settlement>
									<region>KY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adel</forename><forename type="middle">S</forename><surname>Elmaghraby</surname></persName>
							<email>adel.elmaghraby@louisville.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Engineering and Computer Science</orgName>
								<orgName type="institution">University of Louisville</orgName>
								<address>
									<postCode>40292</postCode>
									<settlement>Louisville</settlement>
									<region>KY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammed</forename><surname>Elmogy</surname></persName>
						</author>
						<title level="a" type="main">Classification of pressure ulcer tissues with 3D convolutional neural network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A72602E18FBD173A2A7853D5DAC5609F</idno>
					<idno type="DOI">10.1007/s11517-018-1835-y</idno>
					<note type="submission">Received: 23 October 2017 / Accepted: 19 April 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-08-13T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pressure ulcer</term>
					<term>3D convolution neural network (CNN)</term>
					<term>Tissue classification</term>
					<term>Linear combinations of discrete Gaussians (LCDG)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A 3D convolution neural network (CNN) of deep learning architecture is supplied with essential visual features to accurately classify and segment granulation, necrotic eschar, and slough tissues in pressure ulcer color images. After finding a region of interest (ROI), the features are extracted from both the original and convolved with a pre-selected Gaussian kernel 3D HSI images, combined with first-order models of current and prior visual appearance. The models approximate empirical marginal probability distributions of voxel-wise signals with linear combinations of discrete Gaussians (LCDG). The framework was trained and tested on 193 color pressure ulcer images. The classification accuracy and robustness were evaluated using the Dice similarity coefficient (DSC), the percentage area distance (PAD), and the area under the ROC curve (AUC). The obtained preliminary DSC of 92%, PAD of 13%, and AUC of 95% are promising.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.224" lry="790.955"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.224" lry="790.955"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.224" lry="790.955"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.224" lry="790.955"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.224" lry="790.955"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.224" lry="790.955"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.224" lry="790.955"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.224" lry="790.955"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.224" lry="790.955"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.224" lry="790.955"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.224" lry="790.955"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.224" lry="790.955"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.224" lry="790.955"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.224" lry="790.955"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Chronic wounds (CWs) are considered a significant threat to the public health and economy. They have a harmful effect on patients causing social separation, depression, and high treatment costs <ref type="bibr" target="#b0">[1]</ref>. In the USA, there are approximately 6.5 million patients are influenced by chronic wounds. More than US$25 billion is spent yearly on the treatment of these wounds, because of increasing health care costs, elder people, and the incidence of diabetes and obesity <ref type="bibr" target="#b1">[2]</ref>. CW can be defined as the wound that is remaining unhealed for longer than six weeks. Pressure ulcer (PU), diabetic foot ulcers, and venous ulcers are considered as the essential types of CW. In addition, malignant ulcer, burns, and Pyoderma gangrenosum are still quite significant types of CW. These previously mentioned types are often resistant to healing and required long-term medical care <ref type="bibr" target="#b2">[3]</ref>.</p><p>A PU or ischemic wound is developed due to lingering pressure on the part of the body causing a lack of blood flow to the tissues. In particular, when the body is not moving enough the circulatory system provides less oxygen resulting in an ischemic process under the patient's skin <ref type="bibr" target="#b3">[4]</ref>. Up to 95% of PUs are said to be avoidable, but it is not easy to diagnose the PU until the wound becomes visible due to its advanced state <ref type="bibr" target="#b4">[5]</ref>. In 2013, they caused 29,000 deaths and affected up from 2 to 28% of nursing home residents over the world <ref type="bibr" target="#b5">[6]</ref>, health deterioration usually occurring due to infection when detection and diagnosis are delayed <ref type="bibr" target="#b6">[7]</ref>.</p><p>PUs, also called pressure sores, bed sores <ref type="bibr" target="#b7">[8]</ref>, and pressure injuries, are frequent at support body points coinciding with prominences or bone ridges. The most susceptible sites are sacrum, heel, external and external malleoli, buttocks, trochanters, opplated, isquion, occiput, elbows, iliac crest, ears, spiny apophasis, inner sides and outer faces of knees, and side feet edges. Preventive measures reduce compression and local occlusions of blood capillaries by a shorter stay without movement. Hospital stay times differ widely, and prevalence rates of the pressure ulcers range from 12 to 16% in general acute care, 4.3 to 32% in longterm care, and 2.9 to 19% in home care <ref type="bibr" target="#b8">[9]</ref>. The annual mean prevalence rate decreased from 7.8% in 2005 to 1.4% in 2011, being kept to 2014 <ref type="bibr" target="#b9">[10]</ref>.</p><p>This paper focuses on automated detection of PUs of grades 3 and 4. The grade 3 tissue loss forms a deep crater (sometimes covered with necrotic tissue) through the skin, even reaching the deep dermis and hypodermis. The grade 4 injuries are caves or sinuous routes due to the total skin loss with frequent destruction or tissue necrosis to muscle, bones or support structures like sinus capsule tendon.</p><p>To find the ulcer, one has to describe and locate the lesion, which can be characterized by extension, depth, and color of the tissues. Then, the skin and surrounding tissues are classified by color (pigmented, pale, cyanosis, rosy), texture (rough, thick, thin), turgidity (good, bad), temperature, humidity (dry, wet, normal), edema's degree, and location. This paper considers only on the tissue classification problem. Figure <ref type="figure">1</ref> demonstrates main (granulated, necrotic eschar, and slough) types of PU tissues.</p><p>In literature, PU image analysis is an active field of research. Many studies are conducted to segment and classify different tissue types to help dermatologists evaluating and diagnosing the progress of PU. For example, Dorileo et al. <ref type="bibr" target="#b10">[11]</ref> implemented a PU segmentation system that was based on analyzing both RGB and HIS color spaces of the PU images. They segmented each tissue type depending on choosing a threshold for various color components from the two tested color spaces. They tested the proposed system on 172 PU images and achieved accuracy of 61 ± 25%.</p><p>Vereda et al. <ref type="bibr" target="#b11">[12]</ref> proposed an automatic tissue classification system for wound images based on artificial neural networks (ANNs) and Bayesian classifiers. They used the mean shift and region-growing techniques to segment the area of the ulcer. The color and texture features are supplied to a set of k multilayer perceptrons and Bayesian classifier to classify different types of tissues in the ulcer image. They tested their system on 113 PU images and achieved accuracy of 91.5%.</p><p>Azevedo-Marques et al. <ref type="bibr" target="#b12">[13]</ref> proposed a clustering segmentation method to segment different ulcer tissues. Their method was based on the color components in huesaturation histograms and mathematical morphology. The proposed system was tested on 172 ulcer images. It achieved an average Jaccard coefficient of 56 ± 22% between the resulting segmentation and the ground truth (GT) that was generated by a dermatologist.</p><p>Mukherjee et al. <ref type="bibr" target="#b0">[1]</ref> developed a tissue classification system for CW. First, they transformed the original RGB images to HIS color space to provide higher contrast. The CW images were segmented by using fuzzy divergence based thresholding to minimize edge ambiguity. They classified different CW tissues by using support vector machine (SVM) with 3rd-order polynomial kernel based on the extracted color and textural features. Their system achieved an overall accuracy of 88%.</p><p>Ahmed Fauzi et al. <ref type="bibr" target="#b1">[2]</ref> implemented a segmentation technique for CW color images. First, they generated a red-yellow-black-white (RYKW) probability map to guide the segmentation techniques. The red, yellow, and black probability maps are utilized to handle the granulation, Fig. <ref type="figure">1</ref> Examples of the most common types of tissues in PU images slough, and eschar wound tissues. On the other hand, the white probability map is generated to detect the white label card that was used for measurement calibration aims. Then, the region growing or optimal thresholding is used to segment tissues of the chronic wound. Finally, they ran the experiments on 80 images and achieved an average accuracy of 75.1%.</p><p>Vereda et al. <ref type="bibr" target="#b13">[14]</ref> proposed a tissue segmentation system for PU images based on k-means clustering technique. They used ANN, random forest (RF), and SVM classifiers to distinguish different ulcer tissues. To improve the efficacy of the classifier and decrease the number of necessary predictors, they selected features based on a wrapper approach with recursive feature elimination. RF and SVM classifiers achieved the highest performance values based on extracted features. Their system run on 113 PU images and achieved an average accuracy of 88%.</p><p>Ortiz et al. <ref type="bibr" target="#b14">[15]</ref> proposed a PU segmentation system that is based on toroidal geometry to extract various contrast levels from the proposed images. They used Otsu's threshold technique to segment the images. Then, morphological operators are used to refine the final result. They achieved average accuracy of 89% on 51 PU images.</p><p>On the other hand, some studies used deep learning to segment PU images. For instance, Wang et al. <ref type="bibr" target="#b15">[16]</ref> proposed a wound segmentation technique based on convolution neural network (CNN) model where ConvNet features are used in infection detection via SVM classifier and in healing prediction process via Gaussian Process (GP) Regression. The results showed that ConvNet had a better accuracy compared to SVM classifier. Kawahara and Hamarneh <ref type="bibr" target="#b16">[17]</ref> built a skin lesion segmentation system using multi-track CNN, which extended pertained CNNs for multi-resolution skin lesion classification. The results showed a higher classification accuracy compared to multi-scale approaches. Esteva et al. <ref type="bibr" target="#b17">[18]</ref> classified the skin cancer classification by using Google's Inception v3 CNN architecture pre-trained on the 1000 object classes. The final classification layer is removed and replaced by their data set. The results showed good accuracy for the classification with a level of competencies comparable to dermatologists.</p><p>The main limitations of the current research studies can be summarized in the following points. First, there is a need to improve the accuracy of the automatic classification for both external and internal ulcer boundaries. Most of the current work determined the region of interest (ROI) manually. Second, the objective parameters associated with the morphology of the PU need to be optimally calculated. Finally, the dermatologists need a real-time automatic segmentation system for PU assessment.</p><p>To overcome the first and third limitations, we proposed an automated segmentation system to segment and classify different tissues from PU colored images. The proposed system is based on extracting and fusing different features from ulcer images to provide an accurate ulcer segmentation framework. The extracted feature modalities are provided to a 3D CNN network to segment ulcer tissues depending on various features. The main contributions of the proposed framework can be summarized in the following points. First, we proposed an automatic PU segmentation framework to segment both external and internal ulcer boundaries. Second, the proposed system is based on extracting four different feature types, which are the original image in HSI color space, the convolved image by using 3D Gaussian kernel, the prior appearance model, and the current appearance model using our linear combinations of discrete Gaussians (LCDG). Third, these modalities are fused and supplied to 3D CNN networks with multiple paths to extract the ROI and different ulcer tissues. Finally, we evaluated the performance of the proposed framework by using different metrics to show its effectiveness. The rest of this paper is organized as follows. Section 2 presents the methods that are applied during the presented research to construct the proposed system. Section 3 describes the experimental results. The results are discussed in Section 4. Finally, Section 5 summarizes the conclusion of our work and our future directions of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>This work aims to develop an automatic segmentation system to detect and segment PU RGB-colored images. The main idea of the proposed system depends on extracting different features and representing them as various modalities of PU images. These modalities are fused and supplied to 3D CNN network with multiple paths to extract the ROI and different ulcer tissues. The main architecture of the proposed system is shown in Fig. <ref type="figure">2</ref>. The proposed system consists of two main stages, which are the ROI extraction and tissue segmentation. Figure <ref type="figure">3</ref> illustrates the structure of the 3D CNN network that is used in the ROI extraction stage. It automatically extracts the ROI from the processed image to distinguish the PU area from other patient's skin and background objects. Two different modalities of the processed images are supplied to the 3D CNN network, which are the HSI images and the convolved HSI image with the 3D Gaussian kernel.</p><p>In addition, Fig. <ref type="figure">4</ref> illustrates the structure of the second 3D CNN network that is used in tissue segmentation stage. It segments the resulting ROI to extract different ulcer tissues, which are granulation (red), necrotic eschar (black), and slough (yellow) tissues. Four different modalities of the processed images are supplied to a 3D CNN network. These modalities are the above two HSI modalities in addition to the output of our current appearance model Fig. <ref type="figure">2</ref> The proposed PU segmentation system that is generated by applying a LCDG technique and the prior visual appearance model. The details of the proposed system are discussed in the following subsections. Before discussing the main stages of the proposed system, a list of the mathematical notations, which are utilized throughout the paper, is given below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basic notation</head><formula xml:id="formula_0">Below R = {r = (x, y, z) : 0 ≤ x ≤ X -1, 0 ≤ y ≤ Y -1, 0 ≤ z ≤ Z -1}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The ROI extraction stage</head><p>The goal of this stage is to extract the ROI, which contains the PU area, and build its map, m ROI . This stage uses the initial and convolved ulcer RGB images and converts them into the HSI color images to be supplied as inputs for 3D CNN pathways. The HSI color model eliminates impacts of illumination changes onto chrominance of the images. In addition, the saturation (S) channel of the HSI model provides higher contrast <ref type="bibr" target="#b0">[1]</ref>. Gaussian smoothing (GS) of the initial RGB image is performed by convolution of the image with a moving 3D Gaussian kernel (h σ ). In order to add longer range properties to the original intensities </p><formula xml:id="formula_1">g j = g j * h σ ; j ∈ {R, G, B}<label>(1)</label></formula><p>where g j represents the color channel, j , of the convolved input RGB image, g, and h σ = (h σ :r-r c : (r, r c ) ∈ R 2 ) is a 3D Gaussian kernel with a fixed standard deviation σ <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>;</p><formula xml:id="formula_2">h σ :r-r c = 1 √ 2πσ 2 exp - 1 2σ 2 |r -r c | 2 (2)</formula><p>Here, |r - 2 is the Cartesian distance between the voxel, r, and the kernel's center, r c = (x c , y c , z c ).</p><formula xml:id="formula_3">r c | = (x -x c ) 2 + (y -y c ) 2 + (z -z c )</formula><p>Both the initial RGB image and its convolved version are then converted to the HSI images, g HSI and g GSS , respectively. The Gaussian smoothing helps to reduce noise in the processed images. In particular, it integrates longerrange image properties and eliminates intensity distortions that can affect quality of the captured image. However, it flattens the intensity (I) channel by decreasing its maximal and increasing its minimal values. The image is convolved by the (5 × 5 × 3) kernel with σ 2 = 2.5 to preserve more details. Figure <ref type="figure" target="#fig_2">5</ref> shows examples of ulcer images with their convolved versions.</p><p>The mentioned two models are supplied as separate inputs to a 3D CNN network, which is called DeepMedic <ref type="bibr" target="#b20">[21]</ref>. The DeepMedic network is a 3D CNN with two pathways and seven deep layers (four convolutional layers, two fully connected layers, and one classification layer) to extract the ROI from the processed ulcer images. Each previously extracted modality is presented to the network as an input to one of its pathways. The aim of multiple pathways is to incorporate a larger amount of local contextual information from the ulcer images to extract the ROI. The last layer in the network is a 3D fully connected conditional random field (CRF) layer to enhance the performance of the network and remove false positives (FPs). CRF layer has the main advantage to overcome the limitations of other models, which is handling large neighborhoods in fast inference time. The DeepMedic network uses dense training on the segments that are extracted from the ulcer images to adapt class imbalance of this segmentation problem. The 3D CNN network classifies each voxel in the processed image depending on the local and contextual information of its neighborhood to produce estimates for segmentation labels. It is done by multiple convolutions of the input of each pathway with some filters at the cascaded layers. Each layer consists of feature maps (FMs). Each FM is a group of neurons to detect a specific pattern in the previous layer. The voxel's receptive field is defined as voxels' neighborhood in the input modality that affects the activation of a neuron. Its size increases at each subsequent layer.</p><p>The result of the ROI extraction stage presents the external segmentation of the pressure ulcer image. In other words, the output of this stage introduces the ulcer area that is extracted from other background parts in the processed ulcer image. The output of the CNN is processed by morphological filters to enhance the resulting images. The used morphological filters are filling holes to remove discontinuity in the ulcer area, eliminating small areas, and applying closing operation, respectively. The result (m ROI ) will be provided to the second 3D CNN network in the tissue segmentation stage as ROI input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tissue segmentation stage</head><p>The second stage of our proposed system is the tissue segmentation stage, which classifies the ulcer's tissues into granulation (red), necrotic eschar (black), and slough (yellow) tissues. Figure <ref type="figure">4</ref> illustrates the framework of this stage, which consists of four different modalities for the PU colored image. These modalities are fed as inputs to the 3D DeepMedic CNN network. The architecture of the second 3D CNN consists of eleven layers (eight convolutional layers, two fully connected layers, and one classification layer) with four pathways. Each modality is supplied to one of the four pathways of the 3D CNN network. The ROI (m ROI ), which is the output of the previous stage, and the GT (m GT ) are also supplied to the 3D CNN network. The first two modalities are the same as in the previous stage. In addition, the architecture of the 3D DeepMedic CNN network is the same as used in the previous stage except that it has four pathways instead of two and eleven layers instead of seven layers.</p><p>The third modality (g L ) is generated as an LCDG appearance model of the ulcer intensity image. The convolved intensity channel of the HSI images, which is generated in the last stage, is used to generate this modality. To build this modality, the collected empirical marginal probability distribution of voxel-wise intensities is approximated with a multimodal LCDG, and the latter is separated into the ulcer and background parts in unsupervised mode (see <ref type="bibr" target="#b21">[22]</ref> for details).</p><p>LCDG creates a very close approximation of each distribution related to the mode with a linear combination of sign-alternate discrete Gaussian kernels. Each processed image is presented as a K-model image of dominant modes linked to the resulting image. In our case, K equals to three that represents the types of ulcer's tissues. The probability distributions of the tissues (F s ) are estimated and linked to each mode to segment the processed image by separating the modes:</p><formula xml:id="formula_4">F s = ⎛ ⎝ f s (q) : q ∈ Q; q∈Q f s (q) = 1 ⎞ ⎠<label>(3)</label></formula><p>where q introduces the intensity levels that indicate the empirical marginal probability distribution of levels for m ROI . Therefore, the LCDG of the image is divided into sub-models linked to each dominant mode. The discrete Gaussian (DG) is represented as the probability distribution ( θ ) on Q where each probability (ψ(q|θ)) relates to the cumulative Gaussian probability function ( θ (q)) as follows:</p><formula xml:id="formula_5">θ = (ψ(q|θ) : q ∈ Q) (4)</formula><p>ψ(q, θ) = θ (0.5) for q = 0, θ (q + 0.5)θ (q -0.5) for q = 1, ..., Q -2, 1θ (q -0.5) for q = Q -1.</p><p>(</p><formula xml:id="formula_6">)<label>5</label></formula><p>where θ(μ, σ 2 ), μ is the mean, and σ 2 is the variance. Finally, the LCDG with positive (C p ) and negative (C n ) components is calculated as follows:</p><formula xml:id="formula_7">p w, (q) = C p r=1 w p,r ψ(q|θ p,r ) - C n l=1 w n,l ψ(q|θ n,l ) (6) C p r=1 w p,r - C n l=1 w n,l = 1 (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>where C p ≥ K and its weights w = [w p , w n ] are nonnegative and satisfy Eq. ( <ref type="formula">6</ref>). Finally, we aim to find a Kmodel probability that approximates the unknown marginal intensity level distribution, which can be calculated from:</p><formula xml:id="formula_9">g L = q∈Q f (q)log(p w,θ (q)) (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>where f (q) = (|R|f s (q) + 1)/(|R| + q) that represent its Bayesian estimate <ref type="bibr" target="#b22">[23]</ref>. The model maximizes the expected log-likelihood of the statistically independent empirical data. Figure <ref type="figure">6</ref> shows the step-by-step generation of the LCDG modality for an ulcer image. Finally, the fourth modality (g pr ) is the prior visual appearance model that uses the prior color information and the Euclidean distance to generate the color probability of the ulcer tissues. g pr is constructed by using three main steps. First, databases of the three classes of the ulcer tissues are generated by pixels' HSI values. These values are extracted from manually labeled images by ulcer experts. For each class t ∈ {1, 2, 3}, a database DB t is generated from aggregating the most repeated 100 HSI values of that class from the training labeled images. Second, the Euclidean distances (D r,t ) are calculated between each pixel (r) in the testing image (g) and the three constructed tissue databases (DB t ). Finally, all distances that are greater than or equal to a predefined threshold (D r,t ≥ T) are returned and used to calculate the prior probability for the current pixel:</p><formula xml:id="formula_11">P p,i = N t t∈{1,2,3} N t (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>where t is the tissue class (1 for necrotic eschar class, 2 for granulation class, and 3 slough class) and N is the total number of voxels that have a Euclidean distance greater than or equal the threshold. Figure <ref type="figure">7</ref> shows some examples of ulcer images with their equivalent prior visual appearance images. After generating the four different modalities, they are supplied to the 3D CNN network as individual inputs to each pathway. The GT and ROI, which is generated from the first stage, are also supplied to the CNN network. Table <ref type="table" target="#tab_0">1</ref> lists the main features of the used DeepMedic network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>To test the proposed system, a data set of RGB colored images for PU is provided by IGURCO GESTI ÓN S. L., which is a healthcare services company with many geriatric hospitals and nursing homes in the Basque Country (Spain). The data set contains 36 images with a resolution of 1024 × 1024 pixels, which are captured by using a regular digital camera. In addition, we got 157 images from Medetec wound database that have a resolution of 1024 × 731 pixels <ref type="bibr" target="#b23">[24]</ref>. Three graduate students created the GT for external boundaries and internal ulcer tissues. Finally, the images are refined, validated, and approved by three observers. We implemented our system on a Dell workstation with Intel Xeon CPU E5-2620 v4 at 2.10GH z (2 Quad Processors) and 64GB RAM. All algorithms are implemented by using Matlab, C++, and Python.</p><p>We evaluated the performance of the proposed system by using three common metrics, which are Dice similarity coefficient (DSC) <ref type="bibr" target="#b24">[25]</ref>, percentage area distance (PAD), and area under the curve (AUC). The DSC measures relevant correspondence between two areas regarding their true/false positive and negative parts. The PAD is the relative absolute ulcer difference between the results of the tested model and the GT segmentation. The AUC is the area under the Receiver Operating Characteristic (ROC) curve. It indicates the expectation that a uniformly drawn random positive is ranked before a uniformly drawn random negative. The lower the value of PAD and the higher the values of DSC and AUC, the more accurate of our proposed segmentation system. DSC, PAD, and AUC calculated by using Eqs. <ref type="bibr" target="#b9">(10)</ref>, and <ref type="bibr" target="#b11">(12)</ref>, respectively <ref type="bibr" target="#b25">[26]</ref>.</p><formula xml:id="formula_13">DSC = 200T P 2T P + F P + F N % (<label>10</label></formula><formula xml:id="formula_14">)</formula><formula xml:id="formula_15">P AD = 100 |S a -GT a | GT a % (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>AU C = 0.5</p><formula xml:id="formula_17">T P T P + F N + T N T N + F P (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>where TP (true positive) is the total number of correctly classified pixels of the ulcer; TN (true negative) is the number of the correctly classified pixels of the background; FP (false positive) is the total number of the misclassified pixels of the background; FN (false negative) is the total number of the misclassified pixels of the ulcer, S a is the segmentation area, and GT a is the ground truth area. To validate our results and prevent overfitting, we used two different cross-validation techniques. First, we applied the four-fold cross-validation. Second, we divided the dataset into 60% for training, 10% for validation, and 30% for testing.</p><p>Figure <ref type="figure">8</ref> shows some examples of PU images and their extracted ROI images. It illustrates the original RGB colored image, the GT, the output of the CNN network, and These results show that the first stage of our proposed system can detect the ulcer area automatically with high accuracy.</p><p>To measure the performance of our tissue segmentation system, we compared our proposed system with two other segmentation systems. The first is the segmentation output from the Fuzzy C-Means (FCM) algorithm <ref type="bibr" target="#b26">[27]</ref>. The second is the resulting segmentation from LCDG technique. Figure <ref type="figure">9</ref> shows some examples of the output of tissues segmentation systems. It illustrates the GT and the outputs of all tested systems including the results of our proposed system.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>For the ROI extraction stage, we compared our results with Otsu segmentation technique <ref type="bibr" target="#b27">[28]</ref>. Table <ref type="table" target="#tab_1">2</ref> lists the results of the ROI extraction stage by using Otsu technique and our system. We applied the same morphological filters for both tested methods by using the same parameters. The results of our system are obtained by dividing the data set into 60% for training, 10% for validation, and 30% for testing. Figure <ref type="figure" target="#fig_5">10</ref> shows some examples of PU images and their extracted ROI images by using Otsu technique and our proposed system. For tissue segmentation, we compared our system with two other segmentation techniques, which are FCM and LCDG. Table <ref type="table" target="#tab_2">3</ref>  In addition to the last validation method, we validated our system by using four-fold cross-validation method. It achieved 91 ± 7% for DSC, 14 ± 18% for PAD, and 95 ± 4% for AUC in average for all tissues, which is consistent with the first validation method. On the other hand, we compared the proposed tissue segmentation system using the four suggested modalities with four different 3D CNN networks. Each of these CNN networks is supplied with only one input modality, which are HSI image, GS model, LCDG model, and prior model. The aim of this comparison is to know the effect of fusing the four suggested modalities on the performance of the proposed system. Table <ref type="table">4</ref> lists the experimental results of the testing five 3D CNN networks. Also, we calculated the DSC and AUC for necrotic eschar, granulation, and slough tissues. We noticed that the highest classification scores for the necrotic eschar tissue is from HSI model. On the other hand, the HSI model has the lowest scores for the other tissues. As proposed in our system, we used the four different modalities to have the highest classification score for the three various tissues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>lists the experimental results of the testing</head><p>As mentioned previously, three different experienced observers manually segmented the ulcer images. To validate the GT, the Bland-Altman analysis <ref type="bibr" target="#b28">[29]</ref> is used to assess the degree of agreement between the GTs of the three observers. For good agreement, the mean difference between two GTs of two observers, which is called bias, is near zero. In addition, most of the data points should fall within 95% limits of agreement with the ±1.96 standard deviation (SD). We calculated this statistical analysis to compare the resulting ulcer area for each tissue type by two different observers. The Bland-Altman analysis confirms the robustness of the GTs of the three observers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented an automatic segmentation system to segment granulation, necrotic eschar, and slough tissues from pressure ulcer RGB images. The proposed system consists of two main stages that are ROI extraction and tissue classification. ROI extraction stage automatically extracts the ROI from the processed image to distinguish the PU area from other patient's skin and background objects. Tissue classification stage segments the resulting ROI to extract different ulcer tissues. Four different modalities of the processed images are supplied to a 3D CNN, which are the HSI image and its convolved version by using 3D Gaussian kernel, as well as its first-order current and prior visual appearance models using LCDG. The proposed system was trained and tested on 193 color PU images. The accuracy and robustness of the classifier were evaluated using the DSC, PAD, and AUC. The obtained preliminary results: DSC 92%, PAD 13%, and AUC 95% -are promising. In the future, we will extract some other modalities to increase the performance of the system. Also, we will increase the number of ulcer images to obtain more reliable results. Finally, we will make an assessment to the PU depends on depth images to estimate the grade of the ulcer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>will denote a 3D finite arithmetic lattice of X × Y × Z voxels, r. It supports an RGB color PU image with integer intensity levels, q ∈ Q = {0, 1, . . . , Q -1} in each color channel, g = {g r : r ∈ R, g r ∈ Q}. Binary labels, l ∈ L ROI = {0, 1}, will indicate an extracted ROI (0 for background and 1 for ROI voxels) in a ROI map, m ROI = {m r : r ∈ R, m r ∈ L ROI }, and four color-coded labels, L SEG = {0, 1, 2, 3}, will specify a segmentation map, m SEG = {m r : r ∈ R, m r ∈ L SEG }. Here, the segmentation labels 0, 1, 2, and 3 are for background, necrotic eschar (black), granulation (red), and slough (white &amp; yellow), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 Fig. 4</head><label>34</label><figDesc>Fig. 3 The proposed 3D CNN network for the ROI extraction stage</figDesc><graphic coords="4,178.23,530.39,365.80,183.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 Some examples of the output of the GS model. a The original ulcer images. b The equivalent output of the GS model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 Fig. 7</head><label>67</label><figDesc>Fig. 6 The step-by-step generation of LCDG modality for the colored ulcer image. a The original ulcer image. b The convolved intensity channel of the HSI image. c Estimated tissue classes by LCDG. d The output of the LCDG stage</figDesc><graphic coords="7,178.23,464.69,365.80,249.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 Fig. 9</head><label>89</label><figDesc>Fig. 8 Some examples of the output of the ROI extraction stage. a The original ulcer images. b The manually segmented ROI. c The output of the first 3D CNN. d The final output of the ROI extraction stage after the morphological filter</figDesc><graphic coords="9,84.63,244.10,425.32,445.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10</head><label>10</label><figDesc>Fig. 10 The output of the ROI extraction stage by using Otsu segmentation and our proposed system. The original ulcer images. b The manually segmented ROI. c The output of Otsu segmentation. d The output of our ROI extraction stage</figDesc><graphic coords="11,84.63,60.08,425.32,337.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,84.63,533.69,425.32,165.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="5,51.12,59.78,492.76,358.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,232.23,61.07,311.80,235.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,84.63,60.56,425.32,442.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 The</head><label>1</label><figDesc></figDesc><table><row><cell>main</cell><cell></cell><cell></cell></row><row><cell>characteristics of the 3D CNN</cell><cell>Feature</cell><cell>Value</cell></row><row><cell>for the tissue segmentation stage</cell><cell>Number of layers</cell><cell>11 layers, which are eight convolutional layers (two 30 × 21 3 , two 40 × 17 3 , two 40 × 13 3 , and two 50 × 9 3 ), two fully connected</cell></row><row><cell></cell><cell></cell><cell>layers (150 × 9 3 ), and one classification layer (2 × 9 3 )</cell></row><row><cell></cell><cell>Residual connections</cell><cell>Add the input of layers 3, 5, and 7 to the outputs of layers 4, 6, 8,</cell></row><row><cell></cell><cell></cell><cell>respectively.</cell></row><row><cell></cell><cell>Number of epochs in training</cell><cell>35 epochs with 20 sub-epochs for each one.</cell></row><row><cell></cell><cell>Learning rate</cell><cell>0.001</cell></row><row><cell></cell><cell>Momentum coefficient</cell><cell>0.6</cell></row><row><cell></cell><cell>Rho coefficient</cell><cell>0.9 with epsilonRms = 10 -4</cell></row><row><cell></cell><cell>Regularization</cell><cell>L</cell></row></table><note><p>1 = 0.000001 and L 2 = 0.0001</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>The performance evaluation of the ROI extraction stage</figDesc><table><row><cell>Systems</cell><cell>DSC</cell><cell>PAD</cell><cell>AUC</cell></row><row><cell>Otsu segmentation</cell><cell>80 ± 20</cell><cell>17 ± 17</cell><cell>81 ± 17</cell></row><row><cell>The proposed system</cell><cell>95 ± 10</cell><cell>8 ± 12</cell><cell>95 ± 8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 The</head><label>3</label><figDesc></figDesc><table><row><cell>performance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>evaluation of three different</cell><cell>Systems</cell><cell>Metrics</cell><cell>Necrotic eschar</cell><cell>Granulation</cell><cell>Slough</cell></row><row><cell>systems for tissues</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>segmentation in pressure ulcer</cell><cell>FCM segmentation</cell><cell>DSC</cell><cell>40 ± 37</cell><cell>69 ± 22</cell><cell>71 ± 10</cell></row><row><cell>images</cell><cell></cell><cell>PAD</cell><cell>48 ± 41</cell><cell>35 ± 43</cell><cell>290 ± 102</cell></row><row><cell></cell><cell></cell><cell>AUC</cell><cell>49 ± 29</cell><cell>77 ± 20</cell><cell>81 ± 8</cell></row><row><cell></cell><cell>LCDG segmentation</cell><cell>DSC</cell><cell>68 ± 12</cell><cell>70 ± 25</cell><cell>57 ± 30</cell></row><row><cell></cell><cell></cell><cell>PAD</cell><cell>36 ± 21</cell><cell>59 ± 85</cell><cell>111 ± 49</cell></row><row><cell></cell><cell></cell><cell>AUC</cell><cell>82 ± 8</cell><cell>8 0 ± 15</cell><cell>68 ± 21</cell></row><row><cell></cell><cell>The proposed system</cell><cell>DSC</cell><cell>92 ± 4</cell><cell>9 2 ± 6</cell><cell>9 1 ± 5</cell></row><row><cell></cell><cell></cell><cell>PAD</cell><cell>7 ± 6</cell><cell>1 0 ± 18</cell><cell>22 ± 33</cell></row><row><cell></cell><cell></cell><cell>AUC</cell><cell>93 ± 4</cell><cell>9 6 ± 3</cell><cell>9 5 ± 5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Medical &amp; Biological Engineering &amp; Computing (2018) 56:2245-2258 / Published online: 15 June 2018</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Med Biol Eng Comput (2018) 56:2245-2258</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors thank <rs type="person">Prof. Dr. Georgy Gimel</rs>'farb, <rs type="affiliation">Department of Computer University of Auckland, Auckland, New Zealand</rs>, for his help in revising the paper. In addition, the authors want to thank <rs type="person">Sofia Zahia</rs>, <rs type="person">Connor Burns</rs>, and <rs type="person">Daniel Sierra-Sosa</rs> for their support in summarizing the related work and preparing the masks for the GT images.</p></div>
<div><head>Funding information</head><p>The grants that have contributed with partial funding of the study are I <rs type="grantNumber">T -905 -16</rs> to eVIDA research group from the <rs type="funder">Basque Government</rs>, J <rs type="grantName">C2015 -00305 Josè Castillejo Research Stay Grant</rs> from the <rs type="funder">Spanish Ministry</rs>, and <rs type="grantNumber">ACM2017 09</rs> from the <rs type="funder">University of Deusto</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5zN6DAK">
					<idno type="grant-number">T -905 -16</idno>
					<orgName type="grant-name">C2015 -00305 Josè Castillejo Research Stay Grant</orgName>
				</org>
				<org type="funding" xml:id="_7WRqcVM">
					<idno type="grant-number">ACM2017 09</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bego ña García-Zapirain is an associate professor at University of Deusto faculty. She published more than 35 papers in international scientific jourand presented more than 160 papers in international and national scientific conferences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mohammed Elmogy is an associate professor at</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,58.47,278.29,230.65,7.47;13,65.64,288.28,223.50,7.47;13,65.64,298.28,223.53,7.47;13,65.64,308.27,32.11,7.47" xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated tissue classification framework for reproducible chronic wound assessment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Achar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed Res Int</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,58.47,318.27,230.69,7.47;13,65.64,328.27,223.49,7.47;13,65.64,338.26,223.49,7.47;13,65.64,348.27,223.51,7.47;13,65.64,358.26,84.05,7.47" xml:id="b1">
	<analytic>
		<title level="a" type="main">Computerized segmentation and measurement of chronic wound images</title>
		<author>
			<persName><forename type="first">Mfa</forename><surname>Fauzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Khansa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Catignani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gordillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Gurcan</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0010482515000645" />
	</analytic>
	<monogr>
		<title level="j">Comput Biol Med</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="74" to="85" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,58.47,368.26,230.67,7.47;13,65.64,378.25,223.48,7.47;13,65.64,388.25,223.52,7.47;13,65.64,398.25,223.51,7.47;13,65.64,408.24,208.87,7.47" xml:id="b2">
	<analytic>
		<title level="a" type="main">Human skin wounds: a major and snowballing threat to public health and the economy</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Gordillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kirsner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gottrup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Gurtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Longaker</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1524-475X.2009.00543.x</idno>
		<ptr target="https://doi.org/10.1111/j.1524-475X.2009.00543.x" />
	</analytic>
	<monogr>
		<title level="j">Wound Repair Regen</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="763" to="771" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,58.47,418.24,230.68,7.47;13,65.65,428.23,223.50,7.47;13,65.65,438.24,129.61,7.47" xml:id="b3">
	<analytic>
		<title level="a" type="main">Pressure ulcers in america: prevalence, incidence, and implications for the future</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cuddigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Berlowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Ayello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv Skin Wound Care</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="208" to="215" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,58.47,448.24,230.67,7.47;13,65.65,458.23,223.48,7.47;13,65.65,468.23,223.49,7.47;13,65.65,478.22,223.49,7.47;13,65.65,488.22,223.49,7.47;13,65.65,498.22,31.86,7.47" xml:id="b4">
	<analytic>
		<title level="a" type="main">3d ultrasound elastography for early detection of lesions. evaluation on a pressure ulcer mimicking phantom</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Deprez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cloutier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gehin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dittmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Basset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 29th annual international conference of the ieee engineering in medicine and biology society</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="79" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,58.47,508.21,230.69,7.47;13,65.65,518.21,223.49,7.47;13,65.65,528.21,223.50,7.47;13,65.65,538.21,223.49,7.47;13,65.65,548.20,71.40,7.47" xml:id="b5">
	<analytic>
		<title level="a" type="main">Making health care safer: A critical analysis of patient safety practices</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Agostini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Bogardus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Prevention of Pressure Ulcers in Older Patients</title>
		<imprint>
			<publisher>Agency for Healthcare Research and Quality, U.S. Department of Health and Human Services</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,58.47,558.20,230.65,7.47;13,65.65,568.20,223.53,7.47;13,65.65,578.19,223.48,7.47;13,65.65,588.19,223.52,7.47;13,65.65,598.18,77.20,7.47" xml:id="b6">
	<analytic>
		<title level="a" type="main">A non-contact imaging-based approach to detecting stage i pressure ulcers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leachtenauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Newcomer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the proceedings of the 2006 international conference of the IEEE engineering in medicine and biology society</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="6380" to="6383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,58.47,608.18,230.67,7.47;13,65.65,616.33,223.49,9.32;13,65.65,628.18,69.64,7.47" xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Prado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Andrades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Benítez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Plástica</forename><surname>Cirugía</surname></persName>
		</author>
		<author>
			<persName><surname>Esencial</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="111" to="126" />
		</imprint>
		<respStmt>
			<orgName>Hospital Clinico Universidad De Chile</orgName>
		</respStmt>
	</monogr>
	<note>ch. Úlceras por presión</note>
</biblStruct>

<biblStruct coords="13,58.47,638.18,230.68,7.47;13,65.65,648.17,223.48,7.47;13,65.65,658.17,223.50,7.47;13,65.65,668.16,152.83,7.47" xml:id="b8">
	<analytic>
		<title level="a" type="main">An image mining based approach to detect pressure ulcer stage</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guadagnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Guilhem</surname></persName>
		</author>
		<idno type="DOI">10.1134/S1054661814020084</idno>
		<ptr target="https://doi.org/10.1134/S1054661814020084" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit Image Anal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="292" to="296" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,58.12,678.16,231.01,7.47;13,65.65,688.16,223.50,7.47;13,65.65,698.15,223.49,7.47;13,65.65,708.16,118.14,7.47" xml:id="b9">
	<analytic>
		<title level="a" type="main">Inpatient pressure ulcer prevalence in an acute care hospital using evidence-based practice</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1111/wvn.12145</idno>
		<ptr target="https://doi.org/10.1111/wvn.12145" />
	</analytic>
	<monogr>
		<title level="j">Worldviews Evid-Based Nurs</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="112" to="117" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.24,60.98,231.06,7.47;13,320.76,70.97,223.52,7.47;13,320.76,80.97,163.38,7.47" xml:id="b10">
	<analytic>
		<title level="a" type="main">Segmentation and analysis of the tissue composition of dermatological ulcers</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Dorileo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mac</forename><surname>Frade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Rangayyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Azevedo-Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCECE 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.24,90.97,231.06,7.47;13,320.76,100.97,223.51,7.47;13,320.76,110.96,140.39,7.47" xml:id="b11">
	<analytic>
		<title level="a" type="main">Binary tissue classification on wound images with neural networks and bayesian classifiers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Veredas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Morente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Med Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="410" to="427" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.24,120.96,231.03,7.47;13,320.76,130.96,223.49,7.47;13,320.76,140.95,223.50,7.47;13,320.76,150.95,223.52,7.47;13,320.76,160.94,23.38,7.47" xml:id="b12">
	<analytic>
		<title level="a" type="main">Segmentation of dermatological ulcers using clustering of color components</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Azevedo-Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mac</forename><surname>Frade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Rangayyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Canadian conference on electrical and computer engineering (CCECE)</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.24,170.94,231.02,7.47;13,320.77,180.94,223.52,7.47;13,320.77,190.94,136.69,7.47" xml:id="b13">
	<analytic>
		<title level="a" type="main">Wound image evaluation with machine learning</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Veredas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Luque-Baena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martín-Santos</forename><surname>Fj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morilla</forename><forename type="middle">-</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Morente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="112" to="122" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.25,200.94,231.03,7.47;13,320.77,210.93,223.51,7.47;13,320.77,220.93,223.50,7.47;13,320.77,230.92,99.63,7.47" xml:id="b14">
	<analytic>
		<title level="a" type="main">Pressure ulcer image segmentation technique through synthetic frequencies generation and contrast variation using toroidal geometry</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sierra-Sosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Zapirain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioMedical Engineering OnLine</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.25,240.72,231.04,7.47;13,320.77,250.72,223.52,7.47;13,320.77,260.72,223.53,7.47;13,320.77,270.71,223.50,7.47;13,320.77,280.71,223.51,7.47;13,320.77,290.71,16.99,7.47" xml:id="b15">
	<analytic>
		<title level="a" type="main">A unified framework for automatic wound segmentation and analysis with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kochhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wrobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 37th annual international conference of the ieee engineering in medicine and biology society (EMBC)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2415" to="2418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.25,300.51,231.05,7.47;13,320.77,310.51,223.51,7.47;13,320.77,320.51,151.11,7.47" xml:id="b16">
	<monogr>
		<title level="m" type="main">Multi-resolution-Tract CNN with hybrid pretrained and skin-lesion trained layers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="164" to="171" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.25,330.31,231.02,7.47;13,320.77,340.30,223.49,7.47;13,320.77,350.30,223.52,7.47;13,320.77,360.29,159.32,7.47" xml:id="b17">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuprel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Novoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Swetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature21056</idno>
		<ptr target="https://doi.org/10.1038/nature21056" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="issue">7639</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.25,370.09,231.00,7.47;13,320.77,380.09,223.51,7.47;13,320.77,390.09,223.50,7.47;13,320.77,400.08,223.52,7.47;13,320.77,410.08,12.75,7.47" xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate lungs segmentation on CT chest images by adaptive appearanceguided shape modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Soliman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elnakib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>El-Ghar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dunlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gimel'farb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Keynton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El-Baz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Med Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="276" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.25,419.88,231.05,7.47;13,320.77,429.88,223.50,7.47;13,320.77,439.88,223.53,7.47;13,320.77,449.88,183.85,7.47" xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalized Gaussian scale-space axiomatics comprising linear scale-space, affine scale-space and spatiotemporal scale-space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10851-010-0242-2</idno>
		<ptr target="https://doi.org/10.1007/s10851-010-0242-2" />
	</analytic>
	<monogr>
		<title level="j">J Math Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="81" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.25,459.68,231.04,7.47;13,320.77,469.67,223.49,7.47;13,320.77,478.92,223.53,8.50;13,320.78,489.66,141.28,7.47" xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient multi-scale 3d {CNN} with fully connected {CRF} for accurate brain lesion segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med Image Anal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="61" to="78" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.26,499.46,231.00,7.47;13,320.78,509.46,223.51,7.47;13,320.78,519.46,223.52,7.47;13,320.78,529.45,59.00,7.47" xml:id="b21">
	<analytic>
		<title level="a" type="main">Precise segmentation of 3d magnetic resonance angiography</title>
		<author>
			<persName><forename type="first">A</forename><surname>El-Baz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elnakib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>El-Ghar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcclure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soliman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gimelrfarb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Biomed Eng</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2019" to="2029" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.26,539.25,231.01,7.47;13,320.78,549.25,72.15,7.47" xml:id="b22">
	<monogr>
		<title level="m" type="main">Statistical pattern recognition, 3rd edn</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Copsey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Wiley</publisher>
			<pubPlace>Hoboken</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.26,559.05,231.04,7.47;13,320.79,569.05,144.01,7.47" xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<ptr target="http://www.medetec.co.uk/files/medetec-image-databases.html" />
		<title level="m">Medetec wound database</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.27,578.85,231.05,7.47;13,320.79,588.85,223.51,7.47;13,320.79,598.85,122.54,7.47" xml:id="b24">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Dice</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/1932409" />
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.27,608.65,231.03,7.47;13,320.79,618.64,223.53,7.47;13,320.79,628.64,89.10,7.47" xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluation: from precision, recall and fmeasure to roc, informedness, markedness &amp; correlation</title>
		<author>
			<persName><forename type="first">Dmw</forename><surname>Powers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="63" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.27,638.44,231.02,7.47;13,320.79,648.44,223.51,7.47;13,320.79,658.43,72.95,7.47" xml:id="b26">
	<analytic>
		<title level="a" type="main">Image segmentation by fuzzy c-means clustering algorithm with a novel penalty term</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing and informatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="17" to="31" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.27,668.23,231.02,7.47;13,320.79,678.23,182.67,7.47" xml:id="b27">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Syst Man Cybern</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,313.27,688.03,231.00,7.47;13,320.79,698.02,223.50,7.47;13,320.79,708.02,36.36,7.47" xml:id="b28">
	<analytic>
		<title level="a" type="main">Statistical methods for assessing agreement between two methods of clinical measurement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="307" to="310" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
